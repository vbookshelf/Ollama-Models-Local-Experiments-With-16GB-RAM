{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsE5rhFuCiQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EdY2JYCQePWH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'phi4:14b'\n",
    "\n",
    "# The embeddings and the dataframe created and saved in Part 1\n",
    "PATH_TO_EMBEDS = 'compressed_array.npz'\n",
    "PATH_TO_DF = 'compressed_dataframe.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqxrj_T6C_z1"
   },
   "source": [
    "# Define the API clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Desktop/dot-env-api-keys/my-api-keys.env\"))\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PzAyFOnWC_d9"
   },
   "outputs": [],
   "source": [
    "# Tavily web search\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXpFe86y6DOF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwW76J3yJ5QN"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_message_history(system_message):\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1pl8ksbbeuSD"
   },
   "outputs": [],
   "source": [
    "def create_message_history(system_message, user_input):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a message history messages list.\n",
    "    Args:\n",
    "        system_message (str): The system message\n",
    "        user_query (str): The user input\n",
    "    Returns:\n",
    "        A list of dicts in OpenAi chat format\n",
    "    \"\"\"\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_input\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ywzSfgqFFOkZ"
   },
   "outputs": [],
   "source": [
    "def initialize_message_history(system_message):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a message history messages list.\n",
    "    Args:\n",
    "        system_message (str): The system message\n",
    "        user_query (str): The user input\n",
    "    Returns:\n",
    "        A list of dicts in OpenAi chat format\n",
    "    \"\"\"\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate the thinking and the response\n",
    "\n",
    "def process_response(text):\n",
    "    \n",
    "    text1 = text.split('</think>')[0]\n",
    "    text2 = text.split('</think>')[1]\n",
    "    \n",
    "    thinking_text = text1 + '</think>'\n",
    "    response_text = text2.strip()\n",
    "\n",
    "    return thinking_text, response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent_system_message = f\"\"\"\n",
    "You are a friendly and helpful research assistant named Serena.\n",
    "\n",
    "Your knowledge cutoff: August 2024\n",
    "Current date: August 2025\n",
    "\n",
    "1. You provide polite answers to simple questions.\n",
    "If the user's input requires only a simple answer, then output your answer as JSON.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: Hello. How are you?\n",
    "\n",
    "You output:\n",
    "\n",
    "{{\n",
    "\"Answer\": \"I'm fine, thanks.\",\n",
    "\"Status\": \"DONE\"\n",
    "}}\n",
    "\n",
    "2. You can also run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop, you output an Answer.\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "Output your response as a JSON string.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "find_arxiv_research_papers:\n",
    "e.g. find_arxiv_research_papers: [list of search keywords and phrases for a RAG search of the ArXiv database.]\n",
    "Returns research papers from the ArXiv database.\n",
    "\n",
    "run_web_search:\n",
    "e.g. run_web_search: [list of search keywords and phrases for a web search]\n",
    "Returns text content from search results.\n",
    "\n",
    "You can only call one action at a time.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What are the latest techniques for detecting Pneumonia on x-rays using AI?\n",
    "{{\n",
    "\"Thought\": \"I should look for relevant research papers in the ArXiv database by using find_arxiv_research_papers.\",\n",
    "\"Action\": {{\"function\":\"find_arxiv_research_papers\", \"input\": [\"Pneumonia detection with AI\", \"Computer vision\", \"Object detection\"]}},\n",
    "\"Status\": \"PAUSE\"\n",
    "}}\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: <results>A list of research papers and their content</results>\n",
    "\n",
    "You then output:\n",
    "{{\n",
    "\"Answer\": \"Your final report.\",\n",
    "\"Status\": \"DONE\"\n",
    "}}\n",
    "\n",
    "ALWAYS check your responses to ensure that all required JSON keys and values have been included.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0reljPhbezOM"
   },
   "source": [
    "# Set up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Molly! How can I assist you today? 😊\n"
     ]
    }
   ],
   "source": [
    "def make_llm_api_call(message_history):\n",
    "\n",
    "    model_name = MODEL_NAME\n",
    "\n",
    "    response: ChatResponse = chat(model=model_name, \n",
    "                                  messages=message_history,\n",
    "                                )\n",
    "\n",
    "    output_text = response['message']['content']\n",
    "\n",
    "    #thinking_text, response_text = process_response(output_text)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "system_message = \"Your name is Molly.\"\n",
    "user_message = \"What's your name?\"\n",
    "\n",
    "message_history = create_message_history(system_message, user_message)\n",
    "\n",
    "response_text = make_llm_api_call(message_history)\n",
    "\n",
    "print(response_text)\n",
    "\n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShdYyu_Ne7R2"
   },
   "source": [
    "## Set up the tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiv RAG search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_faiss_search(query_text, top_k):\n",
    "    \n",
    "    # Run FAISS exhaustive search\n",
    "    \n",
    "    query = [query_text]\n",
    "\n",
    "    # Vectorize the query string\n",
    "    query_embedding = sent_model.encode(query)\n",
    "\n",
    "    # Run the query\n",
    "    # index_vals refers to the chunk_list index values\n",
    "    scores, index_vals = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get the list of index vals\n",
    "    index_vals_list = index_vals[0]\n",
    "    \n",
    "    return index_vals_list\n",
    "    \n",
    "\n",
    "def run_rerank(index_vals_list, query_text):\n",
    "    \n",
    "    chunk_list = list(df_data['prepared_text'])\n",
    "\n",
    "    # Replace the chunk index values with the corresponding strings\n",
    "    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n",
    "\n",
    "    # Format the input for the cross encoder\n",
    "    # The input to the cross_encoder is a list of lists\n",
    "    # [[query_text, pred_text1], [query_text, pred_text2], ...]\n",
    "\n",
    "    cross_input_list = []\n",
    "\n",
    "    for item in pred_strings_list:\n",
    "\n",
    "        new_list = [query_text, item]\n",
    "\n",
    "        cross_input_list.append(new_list)\n",
    "\n",
    "\n",
    "    # Put the pred text into a dataframe\n",
    "    df = pd.DataFrame(cross_input_list, columns=['query_text', 'pred_text'])\n",
    "\n",
    "    # Save the orginal index (i.e. df_data index values)\n",
    "    df['original_index'] = index_vals_list\n",
    "\n",
    "    # Now, score all retrieved passages using the cross_encoder\n",
    "    cross_scores = cross_encoder.predict(cross_input_list)\n",
    "\n",
    "    # Add the scores to the dataframe\n",
    "    df['cross_scores'] = cross_scores\n",
    "\n",
    "    # Sort the DataFrame in descending order based on the scores\n",
    "    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n",
    "    \n",
    "    # Reset the index (*This was missed previously*)\n",
    "    df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for i in range(0,len(df_sorted)):\n",
    "\n",
    "        text = df_sorted.loc[i, 'pred_text']\n",
    "\n",
    "        # Get the arxiv id\n",
    "        # original_index refers to the index values in df_filtered\n",
    "        original_index = df_sorted.loc[i, 'original_index']\n",
    "        arxiv_id = df_data.loc[original_index, 'id']\n",
    "        cat_text = df_data.loc[original_index, 'cat_text']\n",
    "        title = df_data.loc[original_index, 'title']\n",
    "\n",
    "        # Crete the link to the research paper pdf\n",
    "        link_to_pdf = f'https://arxiv.org/pdf/{arxiv_id}'\n",
    "\n",
    "        item = {\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'link_to_pdf': link_to_pdf,\n",
    "            'cat_text': cat_text,\n",
    "            'title': title,\n",
    "            'abstract': text\n",
    "        }\n",
    "\n",
    "        pred_list.append(item)\n",
    "\n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def print_search_results(pred_list, num_results_to_print):\n",
    "    \n",
    "    for i in range(0,num_results_to_print):\n",
    "        \n",
    "        pred_dict = pred_list[i]\n",
    "        \n",
    "        link_to_pdf = pred_dict['link_to_pdf']\n",
    "        abstract = pred_dict['abstract']\n",
    "        cat_text = pred_dict['cat_text']\n",
    "        title = pred_dict['title']\n",
    "\n",
    "        print('Title:',title)\n",
    "        print('Categories:',cat_text)\n",
    "        print('Abstract:',abstract)\n",
    "        print('Link to pdf:',link_to_pdf)\n",
    "        print()\n",
    "    \n",
    "   \n",
    "def run_arxiv_search(query_text, top_k=50):\n",
    "    \n",
    "    # Run a faiss greedy search\n",
    "    pred_index_list = run_faiss_search(query_text, top_k)\n",
    "\n",
    "    # This returns a list of dicts with length equal to top_k\n",
    "    pred_list = run_rerank(pred_index_list, query_text)\n",
    "    \n",
    "    # Print the results\n",
    "    #print_search_results(pred_list, num_results_to_print)\n",
    "    \n",
    "    return pred_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2421966, 384)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed array\n",
    "embeddings = np.load(PATH_TO_EMBEDS)\n",
    "\n",
    "# Access the array by the name you specified ('my_array' in this case)\n",
    "embeddings = embeddings['array_data']\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/mv2kfll17l1gh1p7h1nny_940000gn/T/ipykernel_47082/2541750260.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2421966, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>cat_text</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturbati...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>High Energy Physics - Phenomenology</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pe...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>Combinatorics, Computational Geometry</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions {titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is describe...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>General Physics</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle n...</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>Combinatorics</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\Lam...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>Classical Analysis and ODEs, Functional Analysis</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \\\n",
       "0  A fully differential calculation in perturbati...           hep-ph   \n",
       "1  We describe a new algorithm, the $(k,\\ell)$-pe...    math.CO cs.CG   \n",
       "2  The evolution of Earth-Moon system is describe...   physics.gen-ph   \n",
       "3  We show that a determinant of Stirling cycle n...          math.CO   \n",
       "4  In this paper we show how to compute the $\\Lam...  math.CA math.FA   \n",
       "\n",
       "                                           cat_text  \\\n",
       "0               High Energy Physics - Phenomenology   \n",
       "1             Combinatorics, Computational Geometry   \n",
       "2                                   General Physics   \n",
       "3                                     Combinatorics   \n",
       "4  Classical Analysis and ODEs, Functional Analysis   \n",
       "\n",
       "                                       prepared_text  \n",
       "0  Calculation of prompt diphoton production cros...  \n",
       "1  Sparsity-certifying Graph Decompositions {titl...  \n",
       "2  The evolution of the Earth-Moon system based o...  \n",
       "3  A determinant of Stirling cycle numbers counts...  \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed DataFrame\n",
    "\n",
    "df_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "embed_length = embeddings.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(embed_length)\n",
    "\n",
    "# Add the embeddings to the index\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "faiss_index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialize sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sent_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialize the cross_encoder for reranking\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "query_text = \"\"\"\n",
    "I want to build an invisibility cloak like the one in Harry Potter.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# RUN THE SEARCH\n",
    "num_results_to_print = 20 # top_k = 300\n",
    "pred_list = run_arxiv_search(query_text, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': 1101.0904,\n",
       " 'link_to_pdf': 'https://arxiv.org/pdf/1101.0904',\n",
       " 'cat_text': 'Classical Physics',\n",
       " 'title': \"Harry Potter's Cloak\",\n",
       " 'abstract': 'Harry Potter\\'s Cloak {title} The magic \"Harry Potter\\'s cloak\" has been the dream of human beings for really long time. Recently, transformation optics inspired from the advent of metamaterials offers great versatility for manipulating wave propagation at will to create amazing illusion effects. In the present work, we proposed a novel transformation recipe, in which the cloaking shell somehow behaves like a \"cloaking lens\", to provide almost all desired features one can expect for a real magic cloak. The most exciting feature of the current recipe is that an object with arbitrary characteristics (e.g., size, shape or material properties) can be invisibilized perfectly with positive-index materials, which significantly benefits the practical realization of a broad-band cloaking device fabricated with existing materials. Moreover, the one concealed in the hidden region is able to undistortedly communicate with the surrounding world, while the lens-like cloaking shell will protect the cloaked source/sensor from being traced back by outside detectors by creating a virtual image.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who is the current UK Prime Minister?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.parliament.uk/site-information/glossary/prime-minister/', 'title': 'Prime Minister - UK Parliament', 'content': '... leader of the Government. He or she is the leader of the party that wins the most seats at a general election. The current Prime Minister is David Cameron.', 'score': 0.8808076, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom', 'title': 'Prime Minister of the United Kingdom - Wikipedia', 'content': \"Prime Minister of the United Kingdom ; Incumbent Keir Starmer. since 5 July 2024 ; Government of the United Kingdom · Prime Minister's Office\", 'score': 0.87782305, 'raw_content': None}], 'response_time': 1.74, 'request_id': 'a2663d0d-41e2-4bcc-8f13-6b4e406bcc48'}\n"
     ]
    }
   ],
   "source": [
    "def run_tavily_search(query, num_results=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses the Tavily API to run a web search\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "        num_results (int): Num search results\n",
    "    Returns:\n",
    "        tav_response (json string): The search results in json format\n",
    "    \"\"\"\n",
    "\n",
    "    # For basic search:\n",
    "    tav_response = tavily_client.search(query=query, max_results=num_results)\n",
    "\n",
    "    return tav_response\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "query = \"Who is the current UK Prime Minister?\"\n",
    "\n",
    "results = run_tavily_search(query, num_results=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGZoKXk4fDX4"
   },
   "source": [
    "# Set up the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "{\n",
      "  \"Answer\": \"Hi there! I'm fine, thanks for asking. How can I assist you today?\",\n",
      "  \"Status\": \"DONE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_chat_agent(message_history):\n",
    "\n",
    "    print(\"---CHAT AGENT---\")\n",
    "\n",
    "    # Prompt the llm\n",
    "    response = make_llm_api_call(message_history)\n",
    "\n",
    "    response = response.replace('```json', '')\n",
    "    response = response.replace('```', '')\n",
    "    response = response.strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"Hello\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "response_text = run_chat_agent(message_history)\n",
    "\n",
    "print(response_text)\n",
    "\n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NHOvD12NfDHX",
    "outputId": "25d189dc-9de0-4fcc-c198-833ccbd1e470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "to_final_answer\n"
     ]
    }
   ],
   "source": [
    "def run_router_agent(llm_response):\n",
    "\n",
    "    \"\"\"\n",
    "    Route to web search or not.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTER AGENT---\")\n",
    "\n",
    "    # Extract the status\n",
    "    json_response = json.loads(llm_response)\n",
    "    status = json_response['Status']\n",
    "    \n",
    "    print(\"Status:\", status)\n",
    "\n",
    "    if status == 'PAUSE':\n",
    "        print(\"Route: to_research_agent\")\n",
    "        return \"to_research_agent\"\n",
    "\n",
    "    else:\n",
    "        print(\"Route: to_final_answer\")\n",
    "        return \"to_final_answer\"\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"hello\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "# Prompt the chat_agent\n",
    "response = run_chat_agent(message_history)\n",
    "\n",
    "# Run router_agent\n",
    "route = run_router_agent(response)\n",
    "\n",
    "print(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Zj3CJ1q_nGsz",
    "outputId": "dd48f886-b14b-43be-a08d-dd94537ed881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: PAUSE\n",
      "Route: to_research_agent\n",
      "---RESEARCH AGENT---\n",
      "func_to_run: run_web_search\n",
      "func_arg: ['OpenAI new open source models', 'OpenAI model releases']\n",
      "Output: [{'query': 'OpenAI new open source models', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.seangoedecke.com/gpt-oss-is-phi-5/', 'title': \"OpenAI's new open-source model is basically Phi-5\", 'content': \"OpenAI's new open-source model is basically Phi-5 OpenAI's new open-source model is basically Phi-5 ### Phi models and training on synthetic data The big idea behind those models was to train exclusively on synthetic data: instead of text pulled from books or the internet, text generated by other language models or hand-curated textbooks. But since you’re “teaching for the test”, you should expect to do worse than other language models who are training on broad data and end up being good at the benchmarks by accident. Why would OpenAI train Phi-style models, knowing that they’ll perform better on benchmarks than in real-world applications? For OpenAI, it must have been very compelling to train a Phi-style model for their open-source release.\", 'score': 0.8830044, 'raw_content': None}, {'url': 'https://huggingface.co/blog/welcome-openai-gpt-oss', 'title': 'Welcome GPT OSS, the new open-source model family from OpenAI!', 'content': 'Image 20: Some benchmark results of OpenAI GPT OSS models The OpenAI GPT OSS models have been trained extensively to leverage tool use as part of their reasoning efforts. -d \\'{\"input\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 1.0, \"stream\": true, \"model\": \"openai/gpt-oss-120b\"}\\' -d \\'{\"messages\": [{\"role\": \"system\", \"content\": \"hello\"}], \"temperature\": 1.0, \"max_tokens\": 1000, \"stream\": true, \"model\": \"openai/gpt-oss-120b\"}\\' OpenAI GPT OSS is unusual because it distinguishes between a “system” message and a “developer” message at the start of the chat, but most other models only use “system”. In GPT OSS, the system message follows a strict format and contains information like the current date, the model identity and the level of reasoning effort to use, and the “developer” message is more freeform, which makes it (very confusingly) similar to the “system” messages of most other models.', 'score': 0.8306082, 'raw_content': None}, {'url': 'https://www.databricks.com/blog/introducing-openais-new-open-models-databricks', 'title': \"Introducing OpenAI's New Open Models on Databricks\", 'content': '*   gpt-oss on Databricks: Open-weight 20B and 120B models with advanced reasoning and fast, cost-efficient performance. We’re excited to be partnering with OpenAI to launch their open weight models gpt-oss 20B and gpt-oss 120B, which are natively available on Databricks today. gpt-oss joins the growing set of frontier models on Databricks, enabling you to build domain-specific custom AI agents using the best model for your use case while securely leveraging your data with full governance and observability. Use gpt-oss to build intelligent copilots and agents that can securely access your enterprise data. model=\"databricks-gpt-oss-20b\", Fine-tune gpt-oss with your unique enterprise data using Databricks Serverless GPU Compute to optimize the model’s quality for your end use case.', 'score': 0.8266142, 'raw_content': None}, {'url': 'https://www.wired.com/story/openai-just-released-its-first-open-weight-models-since-gpt-2/', 'title': 'OpenAI Just Released Its First Open-Weight Models Since GPT-2', 'content': 'OpenAI Just Released Its First Open-Weight Models Since GPT-2 | WIRED OpenAI Just Released Its First Open-Weight Models Since GPT-2 For OpenAI, they represent a shift away from its recent strategy of focusing on proprietary releases, as the company moves toward a wider and more open group of AI models that are available for users. The last open-weight model released by OpenAI was GPT-2, back in 2019. The two new models from OpenAI are available under the Apache 2.0 license, a popular choice for open-weight models. While the release blog about gpt-oss does not mention DeepSeek or any other Chinese AI company directly, Altman is clear that he wants innovation around open-weight models to happen in the United States.', 'score': 0.82254606, 'raw_content': None}, {'url': 'https://azure.microsoft.com/en-us/blog/openais-open%E2%80%91source-model-gpt%E2%80%91oss-on-azure-ai-foundry-and-windows-ai-foundry/', 'title': \"OpenAI's open‑source model: gpt‑oss on Azure AI Foundry and ...\", 'content': 'OpenAI’s open‑source model: gpt‑oss on Azure AI Foundry and Windows AI Foundry | Microsoft Azure Blog Azure Azure Azure  *   Azure AI Foundry Models *   Azure AI *   Azure AI *   Azure 5.   OpenAI’s open‑source model: gpt‑oss on Azure AI Foundry and Windows AI Foundry *   Azure AI **Azure AI Foundry**provides a unified platform for building, fine-tuning, and deploying intelligent agents with confidence while **Foundry Local**brings open-source models to the edge—enabling flexible, on-device inferencing across billions of devices. *   **Deploy gpt‑oss in the cloud today** with a few CLI commands using Azure AI Foundry.Browse the Azure AI Model Catalog to spin up an endpoint. ### Microsoft Azure AI Foundry Models and Microsoft Security Copilot achieve ISO/IEC 42001:2023 certification *   What is Azure?', 'score': 0.80698997, 'raw_content': None}], 'response_time': 0.88, 'request_id': '92192f95-04fc-4ec2-a0e8-a650e549012e'}, {'query': 'OpenAI model releases', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://help.openai.com/en/articles/9624314-model-release-notes', 'title': 'Model Release Notes - OpenAI Help Center', 'content': '**Introducing GPT-4.1 mini, replacing GPT-4o mini, in ChatGPT for all users (May 14, 2025)** GPT-4.1 mini is a fast, capable, and efficient small model, delivering significant improvements compared to GPT-4o mini—in instruction-following, coding, and overall intelligence. Starting today, GPT-4.1 mini replaces GPT-4o mini in the model picker under \"more models\" for paid users, and will serve as the fallback model for free users once they reach their GPT-4o usage limits. We’ve updated GPT-4o for ChatGPT users on all paid tiers. Today, we’ve updated GPT-4o mini for ChatGPT users on the Free, Plus, and Team tier, along with users that use ChatGPT while logged out. We’re introducing GPT-4o mini, the most capable and cost-efficient small model available today.', 'score': 0.7409441, 'raw_content': None}, {'url': 'https://learn.microsoft.com/en-us/azure/ai-foundry/openai/whats-new', 'title': \"What's new in Azure OpenAI in Azure AI Foundry Models?\", 'content': \"GPT-image-1 (2025-04-15) is the latest image generation model from Azure OpenAI. Customers no longer need to apply for the waitlist to use GPT-4 and GPT-4-32k (the Limited Access registration requirements continue to apply for all Azure OpenAI models). Azure OpenAI now supports the GPT-3.5 Turbo Instruct model. Azure AI Speech also supports OpenAI's Whisper model via the batch transcription API. *   Azure OpenAI On Your Data is now available in preview, enabling you to chat with OpenAI models such as GPT-35-Turbo and GPT-4 and receive responses based on your data. Azure OpenAI now supports image generation APIs powered by OpenAI's DALL-E 2 model. *   **GPT-4 series models are now available in preview on Azure OpenAI**.\", 'score': 0.74004334, 'raw_content': None}, {'url': 'https://openai.com/research/index/release/', 'title': 'OpenAI Research | Release', 'content': '*   ChatGPT(opens in a new window) *   Sora(opens in a new window) *   API Platform(opens in a new window) *   Community(opens in a new window) *   Explore ChatGPT(opens in a new window) *   Download(opens in a new window) *   Help Center(opens in a new window) *   Sora Log in(opens in a new window) Introducing gpt-oss We’re releasing gpt-oss-120b and gpt-oss-20b—two state-of-the-art open-weight language models that deliver strong real-world performance at low cost. *   Explore ChatGPT(opens in a new window) *   Download(opens in a new window) *   Sora log in(opens in a new window) *   API log in(opens in a new window) *   Documentation(opens in a new window) *   Developer Forum(opens in a new window) *   Help Center(opens in a new window)', 'score': 0.69199765, 'raw_content': None}, {'url': 'https://help.openai.com/en/articles/6825453-chatgpt-release-notes', 'title': 'ChatGPT — Release Notes | OpenAI Help Center', 'content': 'Paid users also now have a “Show additional models” toggle in ChatGPT web settings which will add models like o3, o4-mini, 4.1, and GPT-5 Thinking mini. GPT-5 is slowly rolling out to all users on ChatGPT Plus, Pro, Team, and Free plans worldwide across web, mobile, and desktop. GPT-5 in ChatGPT is our next flagship model and the new default for all logged-in users. Record mode is now available to ChatGPT Plus users globally in the macOS desktop app. Enterprise and Edu users can also now choose from the full set of ChatGPT models (GPT-4o, o3, o4-mini and more) when building Custom GPTs. This was launched to Plus, Pro and Team users earlier this month.', 'score': 0.57637566, 'raw_content': None}, {'url': 'https://openai.com/news/', 'title': 'OpenAI News', 'content': '*   ChatGPT(opens in a new window) *   Sora(opens in a new window) *   API Platform(opens in a new window) *   Explore ChatGPT(opens in a new window) *   Sora Log in(opens in a new window) Image 4: GPT-5 Dev > Cover Image Introducing GPT-5 for developers Product Aug 7, 2025 Image 5: GPT-5 Enterprise > Cover Image Image 6: GPT-5 ResearchBlog ArtCard 1x1 Image 7: GPT-5 SystemCard ArtCard 1x1 Image 10: Open Model SystemCard 1x1 Image 11: Open Model ResearchPaper 1x1 Introducing our latest image generation model in the API Product Apr 23, 2025 Image 13: GPT-4.5 *   Explore ChatGPT(opens in a new window) *   Sora log in(opens in a new window) *   API log in(opens in a new window)', 'score': 0.4757764, 'raw_content': None}], 'response_time': 1.33, 'request_id': '9cc5970c-a378-4b16-a297-88a38b6f38bc'}]\n"
     ]
    }
   ],
   "source": [
    "def run_research_agent(llm_response):\n",
    "\n",
    "    print(\"---RESEARCH AGENT---\")\n",
    "\n",
    "    # Extract the status\n",
    "    json_response = json.loads(llm_response)\n",
    "    action_dict = json_response['Action']\n",
    "    func_to_run = action_dict['function']\n",
    "    func_input_list = action_dict['input']\n",
    "    \n",
    "    answer_list = []\n",
    "\n",
    "    if func_to_run == \"find_arxiv_research_papers\":\n",
    "        for search_query in func_input_list:\n",
    "            answer = run_arxiv_search(search_query, top_k=5)\n",
    "            answer_list.append(answer)\n",
    "    else:\n",
    "        for search_query in func_input_list:\n",
    "            answer = run_tavily_search(search_query, num_results=5)\n",
    "            answer_list.append(answer)\n",
    "\n",
    "    print(\"func_to_run:\", func_to_run)\n",
    "    print(\"func_arg:\", func_input_list)\n",
    "    print(\"Output:\", answer_list)\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"Has OpenAi released any new open source models?\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "# Prompt the chat_agent\n",
    "response = run_chat_agent(message_history)\n",
    "\n",
    "# Run router_agent\n",
    "route = run_router_agent(response)\n",
    "\n",
    "\n",
    "if route == \"to_research_agent\":\n",
    "    answer = run_research_agent(response)\n",
    "\n",
    "    # Update message history\n",
    "    #message = {\"role\": \"user\", \"content\": f\"Observation: {answer}\"}\n",
    "    #message_history.append(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output_type(output):\n",
    "    try:\n",
    "        json.loads(output)\n",
    "        return \"is_json\"\n",
    "    except json.JSONDecodeError:\n",
    "        return \"is_plain_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Xrmd29mU7ZLa"
   },
   "outputs": [],
   "source": [
    "def run_final_answer_agent(llm_response):\n",
    "\n",
    "    # Check if the output is JSON\n",
    "    output_type = check_output_type(llm_response)\n",
    "\n",
    "    print(\"---FINAL ANSWER AGENT---\")\n",
    "\n",
    "    if output_type == 'is_json':\n",
    "\n",
    "        json_response = json.loads(llm_response)\n",
    "        final_answer = json_response['Answer']\n",
    "    \n",
    "        print(\"Final answer:\", final_answer)\n",
    "\n",
    "    # The model has ouput plain text that's\n",
    "    # not being formatted as per the system message.\n",
    "    else:\n",
    "        print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6enpJPmzLim"
   },
   "source": [
    "# Run the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lSuoHVFDy8WZ",
    "outputId": "80272366-f203-4680-f6b0-7f4562f46626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: The current year is 2025.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = \"What is the current year?\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_input)\n",
    "\n",
    "for i in range(0,10):\n",
    "\n",
    "    # Prompt the chat_agent\n",
    "    llm_response = run_chat_agent(message_history)\n",
    "    \n",
    "    # Update message history\n",
    "    message = {\"role\": \"assistant\", \"content\": llm_response}\n",
    "    message_history.append(message)\n",
    "\n",
    "    # Run router_agent\n",
    "    route = run_router_agent(llm_response)\n",
    "\n",
    "\n",
    "    if route == \"to_research_agent\":\n",
    "\n",
    "        answer = run_research_agent(llm_response)\n",
    "        \n",
    "        user_input = f\"Observation: {answer}\"\n",
    "        message = {\"role\": \"user\", \"content\": user_input}\n",
    "        message_history.append(message)\n",
    "\n",
    "    else:\n",
    "\n",
    "        run_final_answer_agent(llm_response)\n",
    "\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN_-LCvuNrE_"
   },
   "source": [
    "# Run a chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fZLpFq4tvmG7",
    "outputId": "e01afa16-4c1a-4029-a568-ae6838d2d096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: Hello! How can I assist you today?\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Has OpenAi released any new open source models recently?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: PAUSE\n",
      "Route: to_research_agent\n",
      "---RESEARCH AGENT---\n",
      "func_to_run: run_web_search\n",
      "func_arg: ['OpenAI new open source models release']\n",
      "Output: [{'query': 'OpenAI new open source models release', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://gaiinsights.substack.com/p/openai-releases-two-powerful-open', 'title': 'OpenAI Releases Two Powerful Open Source AI Models', 'content': '# OpenAI Releases Two Powerful Open Source AI Models: What You Need To Know OpenAI Releases Two Powerful Open Source AI Models: What You Need To Know OpenAI’s recent release of two new open source models — gpt-oss-120b and gpt-oss-20b — is a major shift in the AI landscape. Until now, OpenAI’s AI models have been closed source and expensive, especially compared to open source alternatives—particularly those from China, which offer approximately 90% of the performance at 90% lower cost. * Encourage your AI team to test OpenAI’s new open source models, especially if on-premise deployment is a priority. * Audience titles; Board member, CEO, SVP Digital, Chief Innovation Office, CTO, CDO, and AI team leads OpenAI Releases Two Powerful Open Source AI Models: What You Need To Know', 'score': 0.9317675, 'raw_content': None}, {'url': 'https://www.reddit.com/r/OpenAI/comments/1mivca5/openais_new_opensource_model_is_o3_level/', 'title': \"OpenAI's new open-source model is o3 level.\", 'content': \"OpenAI's new open-source model is o3 level. : r/OpenAI Skip to main contentOpenAI's new open-source model is o3 level. : r/OpenAI Image 1: r/OpenAI icon Go to OpenAI r/OpenAI Image 3: r/OpenAI iconr/OpenAIImage 4: OpenAIWhiteImage 5: OpenAIWhite OpenAI's new open-source model is o3 level. Image 6: r/OpenAI - OpenAI's new open-source model is o3 level.Image 7: r/OpenAI - OpenAI's new open-source model is o3 level. Image 8: r/OpenAI - OpenAI's new open-source model is o3 level. Overview of OpenAI's new open-source model o3 Details on OpenAI's open-source projects New to Reddit? *   Games  *   Gaming News & Discussion *   Other Games *   Action Movies & Series *   Animated Movies & Series *   Movie News & Discussion *   About Reddit *   Communities\", 'score': 0.8377398, 'raw_content': None}, {'url': 'https://www.wired.com/story/openai-just-released-its-first-open-weight-models-since-gpt-2/', 'title': 'OpenAI Just Released Its First Open-Weight Models Since ...', 'content': 'OpenAI Just Released Its First Open-Weight Models Since GPT-2 | WIRED OpenAI Just Released Its First Open-Weight Models Since GPT-2 For OpenAI, they represent a shift away from its recent strategy of focusing on proprietary releases, as the company moves toward a wider and more open group of AI models that are available for users. The last open-weight model released by OpenAI was GPT-2, back in 2019. The two new models from OpenAI are available under the Apache 2.0 license, a popular choice for open-weight models. While the release blog about gpt-oss does not mention DeepSeek or any other Chinese AI company directly, Altman is clear that he wants innovation around open-weight models to happen in the United States.', 'score': 0.8351749, 'raw_content': None}, {'url': 'https://www.seangoedecke.com/gpt-oss-is-phi-5/', 'title': \"OpenAI's new open-source model is basically Phi-5\", 'content': \"OpenAI's new open-source model is basically Phi-5 OpenAI's new open-source model is basically Phi-5 ### Phi models and training on synthetic data The big idea behind those models was to train exclusively on synthetic data: instead of text pulled from books or the internet, text generated by other language models or hand-curated textbooks. But since you’re “teaching for the test”, you should expect to do worse than other language models who are training on broad data and end up being good at the benchmarks by accident. Why would OpenAI train Phi-style models, knowing that they’ll perform better on benchmarks than in real-world applications? For OpenAI, it must have been very compelling to train a Phi-style model for their open-source release.\", 'score': 0.8170061, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=BZ0qajzteqA', 'title': \"OpenAI's open source models are finally here\", 'content': 'OpenAI’s open source models are finally here\\nTheo - t3․gg\\n468000 subscribers\\n3034 likes\\n89304 views\\n6 Aug 2025\\nThe open source OpenAI models are finally here, and oh boy were these worth the wait...\\n\\nThank you Agentuity for sponsoring! Check them out at: https://soydev.link/agentuity\\n\\nUse code THANKS-OPENAI for 1 month of T3 chat for $1: https://soydev.link/chat\\n\\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\\n\\nCheck out my Twitch, Twitter, Discord more at https://t3.gg/\\n\\nS/O Ph4se0n3 for the awesome edit 🙏\\n404 comments\\n', 'score': 0.7837407, 'raw_content': None}], 'response_time': 1.09, 'request_id': '179645e0-7d05-4a72-860f-9873f700200a'}]\n",
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: Yes, OpenAI has recently released two new open-source models: gpt-oss-120b and gpt-oss-20b. These releases mark a significant shift in the AI landscape as they are now available under the Apache 2.0 license. This move towards openness contrasts with their previous focus on proprietary models. The release of these models aims to foster innovation and provide an alternative to other open-source options, particularly those originating from China.\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Thanks. I'd like to create an invisibility cloak similar to the one that Harry Potter uses. Can you suggest some research papers that I can consult/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: PAUSE\n",
      "Route: to_research_agent\n",
      "---RESEARCH AGENT---\n",
      "func_to_run: find_arxiv_research_papers\n",
      "func_arg: ['invisibility cloak', 'metamaterials', 'cloaking technology', 'light manipulation']\n",
      "Output: [[{'arxiv_id': 811.0458, 'link_to_pdf': 'https://arxiv.org/pdf/811.0458', 'cat_text': 'Optics', 'title': 'A complementary media invisibility cloak that can cloak objects at a   distance outside the cloaking shell', 'abstract': 'A complementary media invisibility cloak that can cloak objects at a   distance outside the cloaking shell {title} Based on the concept of complementary media, we propose an invisibility cloak operating at a finite frequency that can cloak an object with a pre-specified shape and size within a certain distance outside the shell. The cloak comprises of a dielectric core, and an \"anti-object\" embedded inside a negative index shell. The cloaked object is not blinded by the cloaking shell since it lies outside the cloak. Full-wave simulations in two dimensions have been performed to verify the cloaking effect.'}, {'arxiv_id': 809.2317, 'link_to_pdf': 'https://arxiv.org/pdf/809.2317', 'cat_text': 'Optics, Classical Physics', 'title': 'Invisibility cloak without singularity', 'abstract': 'Invisibility cloak without singularity {title} An elliptical invisible cloak is proposed using a coordinate transformation in the elliptical-cylindrical coordinate system, which crushes the cloaked object to a line segment instead of a point. The elliptical cloak is reduced to a nearly-circular cloak if the elliptical focus becomes very small. The advantage of the proposed invisibility cloak is that none of the parameters is singular and the changing range of all parameters is relatively small.'}, {'arxiv_id': 1412.3294, 'link_to_pdf': 'https://arxiv.org/pdf/1412.3294', 'cat_text': 'Optics', 'title': 'Electrostatic Field Invisibility Cloak', 'abstract': 'Electrostatic Field Invisibility Cloak {title} Invisibility cloak is drawing much attention due to its special camouflage when exposed to physical field varing from wave (electromagnetic field, acoustic field, elastic wave, etc.) to scalar field (thermal field, static magnetic field, dc electric field and mass diffusion). Here, an electrostatic field invisibility cloak has been theoretically investigated, and experimentally demonstrated for the first time to perfectly hide a certain region from sight without disturbing the external electrostatic field. The desired cloaking effect has been achieved via both scattering cancelling technology and transformation optics (TO).This present work will pave a novel way for manipulating of electrostatic field where would enable a wide range of potential applications and sustainable products made available.'}, {'arxiv_id': 1012.2238, 'link_to_pdf': 'https://arxiv.org/pdf/1012.2238', 'cat_text': 'Optics', 'title': 'Macroscopic Invisibility Cloak for Visible Light', 'abstract': 'Macroscopic Invisibility Cloak for Visible Light {title} Invisibility cloaks, a subject that usually occurs in science fiction and myths, have attracted wide interest recently because of their possible realization. The biggest challenge to true invisibility is known to be the cloaking of a macroscopic object in the broad range of wavelengths visible to the human eye. Here we experimentally solve this problem by incorporating the principle of transformation optics into a conventional optical lens fabrication with low-cost materials and simple manufacturing techniques. A transparent cloak made of two pieces of calcite is created. This cloak is able to conceal a macroscopic object with a maximum height of 2 mm, larger than 3500 free-space-wavelength, inside a transparent liquid environment. Its working bandwidth encompassing red, green and blue light is also demonstrated.'}, {'arxiv_id': 810.0263, 'link_to_pdf': 'https://arxiv.org/pdf/810.0263', 'cat_text': 'Mathematical Physics, Analysis of PDEs, Mathematical Physics, Optics', 'title': 'Invisibility and Inverse Problems', 'abstract': 'Invisibility and Inverse Problems {title} This survey of recent developments in cloaking and transformation optics is an expanded version of the lecture by Gunther Uhlmann at the 2008 Annual Meeting of the American Mathematical Society.'}], [{'arxiv_id': 1602.0434, 'link_to_pdf': 'https://arxiv.org/pdf/1602.0434', 'cat_text': 'Optics', 'title': \"Tomorrow's Metamaterials: Manipulation of Electromagnetic Waves in   Space, Time and Spacetime\", 'abstract': \"Tomorrow's Metamaterials: Manipulation of Electromagnetic Waves in   Space, Time and Spacetime {title} Metamaterials represent one of the most vibrant fields of modern science and technology. They are generally dispersive structures in the direct and reciprocal space and time domains. Upon this consideration, I overview here a number of metamaterial innovations developed by colleagues and myself in the holistic framework of space and time dispersion engineering. Moreover, I provide some thoughts regarding the future perspectives of the area.\"}, {'arxiv_id': 2211.03147, 'link_to_pdf': 'https://arxiv.org/pdf/2211.03147', 'cat_text': 'Applied Physics, Optics', 'title': 'Review of foundational concepts and emerging directions in metamaterial   research: Design, phenomena, and applications', 'abstract': 'Review of foundational concepts and emerging directions in metamaterial   research: Design, phenomena, and applications {title} In the past two decades, artificial structures known as metamaterials have been found to exhibit extraordinary material properties that enable the unprecedented manipulation of electromagnetic waves, elastic waves, molecules, and particles. Phenomena such as negative refraction, bandgaps, near perfect wave absorption, wave focusing, negative Poissons ratio, negative thermal conductivity, etc., all are possible with these materials. Metamaterials were originally theorized and fabricated in electrodynamics, but research into their applications has expanded into acoustics, thermodynamics, seismology, classical mechanics, and mass transport. In this Research Update we summarize the history, current state of progress, and emerging directions of metamaterials by field, focusing the unifying principles at the foundation of each discipline. We discuss the different designs and mechanisms behind metamaterials as well as the governing equations and effective material parameters for each field. Also, current and potential applications for metamaterials are discussed. Finally, we provide an outlook on future progress in the emerging field of metamaterials.'}, {'arxiv_id': 1905.0056, 'link_to_pdf': 'https://arxiv.org/pdf/1905.0056', 'cat_text': 'Optics', 'title': 'Spacetime Metamaterials', 'abstract': \"Spacetime Metamaterials {title} This paper presents the authors' vision of the emerging field of spacetime metamaterials in a cohesive and pedagogical perspective. For this purpose, it systematically builds up the physics, modeling and applications of these media upon the foundation of their pure-space and pure-time counterparts.\"}, {'arxiv_id': 1101.0732, 'link_to_pdf': 'https://arxiv.org/pdf/1101.0732', 'cat_text': 'Optics, Materials Science', 'title': 'Hybridization effect in coupled metamaterials', 'abstract': 'Hybridization effect in coupled metamaterials {title} Although the invention of the metamaterials has stimulated the interest of many researchers and possesses many important applications, the basic design idea is very simple: composing effective media from many small structured elements and controlling its artificial EM properties. According to the effective-media model, the coupling interactions between the elements in metamaterials are somewhat ignored; therefore, the effective properties of metamaterials can be viewed as the \"averaged effect\" of the resonance property of the individual elements. However, the coupling interaction between elements should always exist when they are arranged into metamaterials. Sometimes, especially when the elements are very close, this coupling effect is not negligible and will have a substantial effect on the metamaterials\\' properties. In recent years, it has been shown that the interaction between resonance elements in metamaterials could lead to some novel phenomena and interesting applications that do not exist in conventional uncoupled metamaterials. In this paper, we will give a review of these recent developments in coupled metamaterials. For the \"meta-molecule\" composed of several identical resonators, the coupling between these units produces multiple discrete resonance modes due to hybridization. In the case of a \"meta-crystal\" comprising an infinite number of resonators, these multiple discrete resonances can be extended to form a continuous frequency band by strong coupling. This kind of broadband and tunable coupled metamaterial may have interesting applications. Many novel metamaterials and nanophotonic devices could be developed from coupled resonator systems in the future.'}, {'arxiv_id': 1209.5777, 'link_to_pdf': 'https://arxiv.org/pdf/1209.5777', 'cat_text': 'Optics', 'title': 'Optical metamaterials with different metals', 'abstract': 'Optical metamaterials with different metals {title} We investigate the influence of different metals on the electromagnetic response of fishnet metamaterials in the optical regime.We found, instead of using a Drude model, metals with a dielectric function from experimentally measured data should be applied to correctly predict the behavior of optical metamaterials. Through comparison of the performance for fishnet metamaterials made with different metals (i.e., gold, copper, and silver), we found silver is the best choice for the metallic parts compared to other metals, because silver allows for the strongest negative-permeability resonance and, hence, for optical fishnet metamaterials with a high figure-of-merit. Our study offers a valuable reference in the designs for optical metamaterials with optimized properties.'}], [{'arxiv_id': 907.0263, 'link_to_pdf': 'https://arxiv.org/pdf/907.0263', 'cat_text': 'Mathematical Physics, Mathematical Physics', 'title': 'Broadband Exterior Cloaking', 'abstract': \"Broadband Exterior Cloaking {title} It is shown how a recently proposed method of cloaking is effective over a broad range of frequencies. The method is based on three or more active devices. The devices, while not radiating significantly, create a ``quiet zone'' between the devices where the wave amplitude is small. Objects placed within this region are virtually invisible. The cloaking is demonstrated by simulations with a broadband incident pulse.\"}, {'arxiv_id': 2208.09863, 'link_to_pdf': 'https://arxiv.org/pdf/2208.09863', 'cat_text': 'Applied Physics, Optics', 'title': 'Transmittable Nonreciprocal Cloaking', 'abstract': 'Transmittable Nonreciprocal Cloaking {title} Cloaking is typically reciprocal. We introduce here the concept of \\\\emph{transmittable nonreciprocal cloaking} whereby the cloaking system operates as a standard omnidirectional cloak for external illumination, but can transmit light from its center outwards at will. We demonstrate a specific implementation of such cloaking that consists in a set of concentric bianisotropic metasurfaces whose innermost element is nonreciprocal and designed to simultaneously block inward waves and pass -- either omnidirectionaly or directionally -- outward waves. Such cloaking represents a fundamental diversification of conventional cloaking and may find applications in areas such as stealth, blockage avoidance, illusion and cooling.'}, {'arxiv_id': 811.0458, 'link_to_pdf': 'https://arxiv.org/pdf/811.0458', 'cat_text': 'Optics', 'title': 'A complementary media invisibility cloak that can cloak objects at a   distance outside the cloaking shell', 'abstract': 'A complementary media invisibility cloak that can cloak objects at a   distance outside the cloaking shell {title} Based on the concept of complementary media, we propose an invisibility cloak operating at a finite frequency that can cloak an object with a pre-specified shape and size within a certain distance outside the shell. The cloak comprises of a dielectric core, and an \"anti-object\" embedded inside a negative index shell. The cloaked object is not blinded by the cloaking shell since it lies outside the cloak. Full-wave simulations in two dimensions have been performed to verify the cloaking effect.'}, {'arxiv_id': 806.4396, 'link_to_pdf': 'https://arxiv.org/pdf/806.4396', 'cat_text': 'Optics', 'title': 'Hiding Under the Carpet: a New Strategy for Cloaking', 'abstract': 'Hiding Under the Carpet: a New Strategy for Cloaking {title} A new type of cloak is discussed: one that gives all cloaked objects the appearance of a flat conducting sheet. It has the advantage that none of the parameters of the cloak is singular and can in fact be made isotropic. It makes broadband cloaking in the optical frequencies one step closer.'}, {'arxiv_id': 1009.2849, 'link_to_pdf': 'https://arxiv.org/pdf/1009.2849', 'cat_text': 'Optics', 'title': 'Watching outside while under a carpet cloak of invisibility', 'abstract': 'Watching outside while under a carpet cloak of invisibility {title} We demonstrate in this letter a unique approach for watching outside while hiding in a carpet cloaking based on transformation optics. Unlike conventional carpet cloaking, which screens all the incident electromagnetic waves, we break the cloak and allow incident light get into the carpet. Hence outside information is detected inside the cloak. To recover the invisible cloaking, complementary techniques are applied in the broken space. Consequently, a hiding-inside-and-watching-outside (HIWO) carpet cloak is sewed, which works as a perfectly invisible cloaking and allows surveillance of the outside at the same time. Our work provides a strategy for ideal cloak with \"hiding\" and \"watching\" functions simultaneously.'}], [{'arxiv_id': 1312.6903, 'link_to_pdf': 'https://arxiv.org/pdf/1312.6903', 'cat_text': 'Optics', 'title': 'Manipulating light at distance by a metasurface using momentum   transformation', 'abstract': 'Manipulating light at distance by a metasurface using momentum   transformation {title} A momentum conservation approach is introduced to manipulate light at distance using metasurfaces. Given a specified field existing on one side of the metasurface and specified desired field transmitted from the opposite side, a general momentum boundary condition is established, which determines the amplitude, phase and polarization transformation to be induced by the metasurface. This approach, named momentum transformation, enables a systematic way to synthesize metasurfaces with complete control over the reflected and transmitted fields. Several synthesis illustrative examples are provided: a vortex hypergeometric-Gaussian beam and a \"delayed-start\" accelerated beam for Fresnel region manipulation, and a pencil beam radiator and a holographic repeater for Frauenhofer region manipulation.'}, {'arxiv_id': 2104.0355, 'link_to_pdf': 'https://arxiv.org/pdf/2104.0355', 'cat_text': 'Optics', 'title': 'Roadmap on multimode light shaping', 'abstract': 'Roadmap on multimode light shaping {title} Our ability to generate new distributions of light has been remarkably enhanced in recent years. At the most fundamental level, these light patterns are obtained by ingeniously combining different electromagnetic modes. Interestingly, the modal superposition occurs in the spatial, temporal as well as spatio-temporal domain. This generalized concept of structured light is being applied across the entire spectrum of optics: generating classical and quantum states of light, harnessing linear and nonlinear light-matter interactions, and advancing applications in microscopy, spectroscopy, holography, communication, and synchronization. This Roadmap highlights the common roots of these different techniques and thus establishes links between research areas that complement each other seamlessly. We provide an overview of all these areas, their backgrounds, current research, and future developments. We highlight the power of multimodal light manipulation and want to inspire new eclectic approaches in this vibrant research community.'}, {'arxiv_id': 2004.10196, 'link_to_pdf': 'https://arxiv.org/pdf/2004.10196', 'cat_text': 'Optics', 'title': 'Lossless reshaping of structured light', 'abstract': 'Lossless reshaping of structured light {title} Structured light concerns the control of light in its spatial degrees of freedom (amplitude, phase and polarization), and has proven instrumental in many applications. The creation of structured light usually involves the conversion of a Gaussian mode to a desired structure in a single step, while the detection is often the reverse process, both fundamentally lossy or imperfect. Here we show how to ideally reshape structured light in a lossless manner in a simple two-step process. We outline the core theoretical arguments, and demonstrate reshaping of arbitrary structured light patterns, in the process highlighting when the technique is applicable and when not, and how best to implement it. This work will be a useful addition to the structured light toolkit, and particularly relevant to those wishing to use the spatial modes of light as a basis in classical and quantum communication.'}, {'arxiv_id': 2003.144, 'link_to_pdf': 'https://arxiv.org/pdf/2003.144', 'cat_text': 'Optics, Applied Physics, Quantum Physics', 'title': 'Integrated Structured Light Architectures', 'abstract': 'Integrated Structured Light Architectures {title} The structural versatility of light underpins an outstanding collection of optical phenomena where both geometrical and topological states of light can dictate how matter will respond or display. Light possesses multiple degrees of freedom such as amplitude, and linear, spin angular, and orbital angular momenta, but the ability to adaptively engineer the spatio-temporal distribution of all these characteristics is primarily curtailed by technologies used to impose any desired structure to light. We describe a foundational demonstration that examines a laser architecture offering integrated spatio-temporal field control and programmability, thereby presenting unique opportunities for generating light by design to exploit its topology.'}, {'arxiv_id': 2002.09663, 'link_to_pdf': 'https://arxiv.org/pdf/2002.09663', 'cat_text': 'Computer Vision and Pattern Recognition', 'title': 'Active Lighting Recurrence by Parallel Lighting Analogy for Fine-Grained   Change Detection', 'abstract': 'Active Lighting Recurrence by Parallel Lighting Analogy for Fine-Grained   Change Detection {title} This paper studies a new problem, namely active lighting recurrence (ALR) that physically relocalizes a light source to reproduce the lighting condition from single reference image for a same scene, which may suffer from fine-grained changes during twice observations. ALR is of great importance for fine-grained visual inspection and change detection, because some phenomena or minute changes can only be clearly observed under particular lighting conditions. Therefore, effective ALR should be able to online navigate a light source toward the target pose, which is challenging due to the complexity and diversity of real-world lighting and imaging processes. To this end, we propose to use the simple parallel lighting as an analogy model and based on Lambertian law to compose an instant navigation ball for this purpose. We theoretically prove the feasibility, i.e., equivalence and convergence, of this ALR approach for realistic near point light source and small near surface light source. Besides, we also theoretically prove the invariance of our ALR approach to the ambiguity of normal and lighting decomposition. The effectiveness and superiority of the proposed approach have been verified by both extensive quantitative experiments and challenging real-world tasks on fine-grained change detection of cultural heritages. We also validate the generality of our approach to non-Lambertian scenes.'}]]\n",
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m message_history\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run router_agent\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m route \u001b[38;5;241m=\u001b[39m \u001b[43mrun_router_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m route \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_research_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     40\u001b[0m     answer \u001b[38;5;241m=\u001b[39m run_research_agent(llm_response)\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mrun_router_agent\u001b[0;34m(llm_response)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---ROUTER AGENT---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract the status\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m json_response \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m status \u001b[38;5;241m=\u001b[39m json_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus:\u001b[39m\u001b[38;5;124m\"\u001b[39m, status)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.17-macos-aarch64-none/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.17-macos-aarch64-none/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.17-macos-aarch64-none/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# NOTES\n",
    "# 1. The model thinking is being printed out.\n",
    "\n",
    "\n",
    "# Initialize the message history\n",
    "message_history = initialize_message_history(chat_agent_system_message)\n",
    "\n",
    "while True:\n",
    "\n",
    "    print()\n",
    "    print(\"==========\")\n",
    "    user_input = input(\"Enter something (or 'q' to quit): \")\n",
    "    print(\"==========\")\n",
    "\n",
    "    # Update message history\n",
    "    message = {\"role\": \"user\", \"content\": user_input}\n",
    "    message_history.append(message)\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"Exiting the loop. Goodbye!\")\n",
    "        break  # Exit the loop\n",
    "\n",
    "\n",
    "    for i in range(0,10):\n",
    "           \n",
    "        llm_response = run_chat_agent(message_history)\n",
    "        \n",
    "        # Update message history\n",
    "        message = {\"role\": \"assistant\", \"content\": llm_response}\n",
    "        message_history.append(message)\n",
    "\n",
    "        # Run router_agent\n",
    "        route = run_router_agent(llm_response)\n",
    "            \n",
    "\n",
    "        if route == \"to_research_agent\":\n",
    "\n",
    "            answer = run_research_agent(llm_response)\n",
    "            \n",
    "            user_input = f\"Observation: {answer}\"\n",
    "            message = {\"role\": \"user\", \"content\": user_input}\n",
    "            message_history.append(message)\n",
    "\n",
    "        else:\n",
    "\n",
    "            run_final_answer_agent(llm_response)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "pYqL1V1TJ_Yq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text appears to be a collection of summaries for various academic papers related to optics, particularly focusing on concepts like cloaking, structured light, and metasurfaces. Below is an overview of the topics covered in these summaries:\n",
      "\n",
      "1. **Complementary Media Cloak**: This involves designing an invisibility cloak that can hide objects situated at a distance from the cloaking shell. It uses complementary media theories to achieve this effect, verified through full-wave simulations.\n",
      "\n",
      "2. **Transmittable Nonreciprocal Cloaking**: A novel type of cloaking that allows for bidirectional control over light transmission; it acts as an omnidirectional cloak externally but can selectively transmit light outward from its center.\n",
      "\n",
      "3. **Momentum Transformation in Metasurfaces**: This method uses momentum conservation principles to manipulate light at a distance using metasurfaces, enabling precise control over reflected and transmitted fields.\n",
      "\n",
      "4. **Multimode Light Shaping Roadmap**: A comprehensive overview of techniques for generating structured light, with applications across various domains like microscopy and communication.\n",
      "\n",
      "5. **Lossless Reshaping of Structured Light**: Presents a two-step method for reshaping light patterns without loss, improving the efficiency and precision in controlling spatial modes of light.\n",
      "\n",
      "6. **Integrated Structured Light Architectures**: Discusses an architecture that integrates spatio-temporal control over light, enhancing its utility in exploiting optical phenomena based on light's topology.\n",
      "\n",
      "7. **Active Lighting Recurrence (ALR)**: Focuses on recreating specific lighting conditions to detect fine-grained changes in a scene, using parallel lighting models and navigation algorithms for active source positioning.\n",
      "\n",
      "These summaries highlight cutting-edge research in the field of optics, particularly focusing on novel methods for manipulating light and its applications in technology and science. Each paper addresses distinct challenges and proposes innovative solutions or frameworks within their respective areas of study.\n"
     ]
    }
   ],
   "source": [
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 8701001,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 164070052,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
