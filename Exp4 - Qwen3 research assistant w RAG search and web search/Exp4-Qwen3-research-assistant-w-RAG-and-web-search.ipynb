{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsE5rhFuCiQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EdY2JYCQePWH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'qwen3:4b' # qwen3:4b, qwen3:8b, qwen3:14b\n",
    "\n",
    "# The embeddings and the dataframe created and saved in Part 1\n",
    "PATH_TO_EMBEDS = 'compressed_array.npz'\n",
    "PATH_TO_DF = 'compressed_dataframe.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqxrj_T6C_z1"
   },
   "source": [
    "# Define the API clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Desktop/dot-env-api-keys/my-api-keys.env\"))\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PzAyFOnWC_d9"
   },
   "outputs": [],
   "source": [
    "# Tavily web search\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXpFe86y6DOF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwW76J3yJ5QN"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_message_history(system_message):\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "1pl8ksbbeuSD"
   },
   "outputs": [],
   "source": [
    "def create_message_history(system_message, user_input):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a message history messages list.\n",
    "    Args:\n",
    "        system_message (str): The system message\n",
    "        user_query (str): The user input\n",
    "    Returns:\n",
    "        A list of dicts in OpenAi chat format\n",
    "    \"\"\"\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_input\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "ywzSfgqFFOkZ"
   },
   "outputs": [],
   "source": [
    "def initialize_message_history(system_message):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a message history messages list.\n",
    "    Args:\n",
    "        system_message (str): The system message\n",
    "        user_query (str): The user input\n",
    "    Returns:\n",
    "        A list of dicts in OpenAi chat format\n",
    "    \"\"\"\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate the thinking and the response\n",
    "\n",
    "def process_response(text):\n",
    "    \n",
    "    text1 = text.split('</think>')[0]\n",
    "    text2 = text.split('</think>')[1]\n",
    "    \n",
    "    thinking_text = text1 + '</think>'\n",
    "    response_text = text2.strip()\n",
    "\n",
    "    return thinking_text, response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent_system_message = f\"\"\"\n",
    "You are a helpful research assistant.\n",
    "\n",
    "Your knowledge cutoff date: October 2023\n",
    "Current date: August 2025\n",
    "\n",
    "1. You provide polite answers to simple questions.\n",
    "If the user's input requires only a simple answer, then output your answer as JSON.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: Hello. How are you?\n",
    "\n",
    "You output:\n",
    "\n",
    "{{\n",
    "\"Answer\": \"I'm fine, thanks.\",\n",
    "\"Status\": \"DONE\"\n",
    "}}\n",
    "\n",
    "2. You can also run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop, you output an Answer.\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "Output your response as a JSON string.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "find_arxiv_research_papers:\n",
    "e.g. find_arxiv_research_papers: [list of search keywords and phrases for a RAG search of the ArXiv database.]\n",
    "Returns research papers from the ArXiv database.\n",
    "\n",
    "run_web_search:\n",
    "e.g. run_web_search: [list of search keywords and phrases for a web search]\n",
    "Returns text content from search results.\n",
    "\n",
    "You can only call one action at a time.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What are the latest techniques for detecting Pneumonia on x-rays using AI?\n",
    "{{\n",
    "\"Thought\": \"I should look for relevant research papers in the ArXiv database by using find_arxiv_research_papers.\",\n",
    "\"Action\": {{\"function\":\"find_arxiv_research_papers\", \"input\": [\"Pneumonia detection with AI\", \"Computer vision\", \"Object detection\"]}},\n",
    "\"Status\": \"PAUSE\"\n",
    "}}\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: <results>A list of research papers and their content</results>\n",
    "\n",
    "You then output:\n",
    "{{\n",
    "\"Answer\": \"Your final report.\",\n",
    "\"Status\": \"DONE\"\n",
    "}}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0reljPhbezOM"
   },
   "source": [
    "# Set up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Molly. How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "def make_llm_api_call(message_history):\n",
    "\n",
    "    model_name = 'qwen3:4b'\n",
    "\n",
    "    response: ChatResponse = chat(model=model_name, \n",
    "                                  messages=message_history,\n",
    "                                )\n",
    "\n",
    "    output_text = response['message']['content']\n",
    "\n",
    "    thinking_text, response_text = process_response(output_text)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "system_message = \"Your name is Molly.\"\n",
    "user_message = \"What's your name?\"\n",
    "\n",
    "message_history = create_message_history(system_message, user_message)\n",
    "\n",
    "response_text = make_llm_api_call(message_history)\n",
    "\n",
    "print(response_text)\n",
    "\n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShdYyu_Ne7R2"
   },
   "source": [
    "## Set up the tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiv RAG search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_faiss_search(query_text, top_k):\n",
    "    \n",
    "    # Run FAISS exhaustive search\n",
    "    \n",
    "    query = [query_text]\n",
    "\n",
    "    # Vectorize the query string\n",
    "    query_embedding = sent_model.encode(query)\n",
    "\n",
    "    # Run the query\n",
    "    # index_vals refers to the chunk_list index values\n",
    "    scores, index_vals = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get the list of index vals\n",
    "    index_vals_list = index_vals[0]\n",
    "    \n",
    "    return index_vals_list\n",
    "    \n",
    "\n",
    "def run_rerank(index_vals_list, query_text):\n",
    "    \n",
    "    chunk_list = list(df_data['prepared_text'])\n",
    "\n",
    "    # Replace the chunk index values with the corresponding strings\n",
    "    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n",
    "\n",
    "    # Format the input for the cross encoder\n",
    "    # The input to the cross_encoder is a list of lists\n",
    "    # [[query_text, pred_text1], [query_text, pred_text2], ...]\n",
    "\n",
    "    cross_input_list = []\n",
    "\n",
    "    for item in pred_strings_list:\n",
    "\n",
    "        new_list = [query_text, item]\n",
    "\n",
    "        cross_input_list.append(new_list)\n",
    "\n",
    "\n",
    "    # Put the pred text into a dataframe\n",
    "    df = pd.DataFrame(cross_input_list, columns=['query_text', 'pred_text'])\n",
    "\n",
    "    # Save the orginal index (i.e. df_data index values)\n",
    "    df['original_index'] = index_vals_list\n",
    "\n",
    "    # Now, score all retrieved passages using the cross_encoder\n",
    "    cross_scores = cross_encoder.predict(cross_input_list)\n",
    "\n",
    "    # Add the scores to the dataframe\n",
    "    df['cross_scores'] = cross_scores\n",
    "\n",
    "    # Sort the DataFrame in descending order based on the scores\n",
    "    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n",
    "    \n",
    "    # Reset the index (*This was missed previously*)\n",
    "    df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for i in range(0,len(df_sorted)):\n",
    "\n",
    "        text = df_sorted.loc[i, 'pred_text']\n",
    "\n",
    "        # Get the arxiv id\n",
    "        # original_index refers to the index values in df_filtered\n",
    "        original_index = df_sorted.loc[i, 'original_index']\n",
    "        arxiv_id = df_data.loc[original_index, 'id']\n",
    "        cat_text = df_data.loc[original_index, 'cat_text']\n",
    "        title = df_data.loc[original_index, 'title']\n",
    "\n",
    "        # Crete the link to the research paper pdf\n",
    "        link_to_pdf = f'https://arxiv.org/pdf/{arxiv_id}'\n",
    "\n",
    "        item = {\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'link_to_pdf': link_to_pdf,\n",
    "            'cat_text': cat_text,\n",
    "            'title': title,\n",
    "            'abstract': text\n",
    "        }\n",
    "\n",
    "        pred_list.append(item)\n",
    "\n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def print_search_results(pred_list, num_results_to_print):\n",
    "    \n",
    "    for i in range(0,num_results_to_print):\n",
    "        \n",
    "        pred_dict = pred_list[i]\n",
    "        \n",
    "        link_to_pdf = pred_dict['link_to_pdf']\n",
    "        abstract = pred_dict['abstract']\n",
    "        cat_text = pred_dict['cat_text']\n",
    "        title = pred_dict['title']\n",
    "\n",
    "        print('Title:',title)\n",
    "        print('Categories:',cat_text)\n",
    "        print('Abstract:',abstract)\n",
    "        print('Link to pdf:',link_to_pdf)\n",
    "        print()\n",
    "    \n",
    "   \n",
    "def run_arxiv_search(query_text, top_k=50):\n",
    "    \n",
    "    # Run a faiss greedy search\n",
    "    pred_index_list = run_faiss_search(query_text, top_k)\n",
    "\n",
    "    # This returns a list of dicts with length equal to top_k\n",
    "    pred_list = run_rerank(pred_index_list, query_text)\n",
    "    \n",
    "    # Print the results\n",
    "    #print_search_results(pred_list, num_results_to_print)\n",
    "    \n",
    "    return pred_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2421966, 384)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed array\n",
    "embeddings = np.load(PATH_TO_EMBEDS)\n",
    "\n",
    "# Access the array by the name you specified ('my_array' in this case)\n",
    "embeddings = embeddings['array_data']\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2421966, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/mv2kfll17l1gh1p7h1nny_940000gn/T/ipykernel_29005/2541750260.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>cat_text</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturbati...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>High Energy Physics - Phenomenology</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pe...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>Combinatorics, Computational Geometry</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions {titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is describe...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>General Physics</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle n...</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>Combinatorics</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\Lam...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>Classical Analysis and ODEs, Functional Analysis</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \\\n",
       "0  A fully differential calculation in perturbati...           hep-ph   \n",
       "1  We describe a new algorithm, the $(k,\\ell)$-pe...    math.CO cs.CG   \n",
       "2  The evolution of Earth-Moon system is describe...   physics.gen-ph   \n",
       "3  We show that a determinant of Stirling cycle n...          math.CO   \n",
       "4  In this paper we show how to compute the $\\Lam...  math.CA math.FA   \n",
       "\n",
       "                                           cat_text  \\\n",
       "0               High Energy Physics - Phenomenology   \n",
       "1             Combinatorics, Computational Geometry   \n",
       "2                                   General Physics   \n",
       "3                                     Combinatorics   \n",
       "4  Classical Analysis and ODEs, Functional Analysis   \n",
       "\n",
       "                                       prepared_text  \n",
       "0  Calculation of prompt diphoton production cros...  \n",
       "1  Sparsity-certifying Graph Decompositions {titl...  \n",
       "2  The evolution of the Earth-Moon system based o...  \n",
       "3  A determinant of Stirling cycle numbers counts...  \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed DataFrame\n",
    "\n",
    "df_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "embed_length = embeddings.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(embed_length)\n",
    "\n",
    "# Add the embeddings to the index\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "faiss_index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialize sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sent_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialize the cross_encoder for reranking\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "query_text = \"\"\"\n",
    "I want to build an invisibility cloak like the one in Harry Potter.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# RUN THE SEARCH\n",
    "num_results_to_print = 20 # top_k = 300\n",
    "pred_list = run_arxiv_search(query_text, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': 1101.0904,\n",
       " 'link_to_pdf': 'https://arxiv.org/pdf/1101.0904',\n",
       " 'cat_text': 'Classical Physics',\n",
       " 'title': \"Harry Potter's Cloak\",\n",
       " 'abstract': 'Harry Potter\\'s Cloak {title} The magic \"Harry Potter\\'s cloak\" has been the dream of human beings for really long time. Recently, transformation optics inspired from the advent of metamaterials offers great versatility for manipulating wave propagation at will to create amazing illusion effects. In the present work, we proposed a novel transformation recipe, in which the cloaking shell somehow behaves like a \"cloaking lens\", to provide almost all desired features one can expect for a real magic cloak. The most exciting feature of the current recipe is that an object with arbitrary characteristics (e.g., size, shape or material properties) can be invisibilized perfectly with positive-index materials, which significantly benefits the practical realization of a broad-band cloaking device fabricated with existing materials. Moreover, the one concealed in the hidden region is able to undistortedly communicate with the surrounding world, while the lens-like cloaking shell will protect the cloaked source/sensor from being traced back by outside detectors by creating a virtual image.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who is the current UK Prime Minister?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom', 'title': 'Prime Minister of the United Kingdom - Wikipedia', 'content': \"Prime Minister of the United Kingdom ; Incumbent Keir Starmer. since 5 July 2024 ; Government of the United Kingdom Â· Prime Minister's Office\", 'score': 0.87782305, 'raw_content': None}, {'url': 'https://www.ox.ac.uk/about/oxford-people/british-prime-ministers', 'title': 'British Prime Ministers | University of Oxford', 'content': 'British Prime Ministers ; Sir Keir Starmer (1962- ), St Edmund Hall, Jul 2024-, Labour ; Rishi Sunak (1980- ), Lincoln, Oct 2022-Jul 2024, Conservative.', 'score': 0.8051581, 'raw_content': None}], 'response_time': 0.9, 'request_id': '6e71a307-c656-4865-9ec3-326dc5173277'}\n"
     ]
    }
   ],
   "source": [
    "def run_tavily_search(query, num_results=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses the Tavily API to run a web search\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "        num_results (int): Num search results\n",
    "    Returns:\n",
    "        tav_response (json string): The search results in json format\n",
    "    \"\"\"\n",
    "\n",
    "    # For basic search:\n",
    "    tav_response = tavily_client.search(query=query, max_results=num_results)\n",
    "\n",
    "    return tav_response\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "query = \"Who is the current UK Prime Minister?\"\n",
    "\n",
    "results = run_tavily_search(query, num_results=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGZoKXk4fDX4"
   },
   "source": [
    "# Set up the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user said \"Hello\". I need to respond politely. Let me check the guidelines. The first example shows a simple answer. So I should just say \"I'm fine, thanks.\" and mark it as DONE. No need for any actions here. Just a straightforward reply.\n",
      "</think>\n",
      "{\n",
      "\"Answer\": \"I'm fine, thanks.\",\n",
      "\"Status\": \"DONE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_chat_agent(message_history):\n",
    "\n",
    "    model_name = MODEL_NAME\n",
    "\n",
    "    response: ChatResponse = chat(model=model_name, \n",
    "                                  messages=message_history,\n",
    "                                )\n",
    "\n",
    "    output_text = response['message']['content']\n",
    "\n",
    "    thinking_text, response_text = process_response(output_text)\n",
    "\n",
    "    print(thinking_text)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"Hello\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "response_text = run_chat_agent(message_history)\n",
    "\n",
    "print(response_text)\n",
    "\n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "NHOvD12NfDHX",
    "outputId": "25d189dc-9de0-4fcc-c198-833ccbd1e470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "to_final_answer\n"
     ]
    }
   ],
   "source": [
    "def run_router_agent(llm_response):\n",
    "\n",
    "    \"\"\"\n",
    "    Route to web search or not.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTER AGENT---\")\n",
    "\n",
    "    # Extract the status\n",
    "    json_response = json.loads(llm_response)\n",
    "    status = json_response['Status']\n",
    "    \n",
    "    print(\"Status:\", status)\n",
    "\n",
    "    if status == 'PAUSE':\n",
    "        print(\"Route: to_research_agent\")\n",
    "        return \"to_research_agent\"\n",
    "\n",
    "    else:\n",
    "        print(\"Route: to_final_answer\")\n",
    "        return \"to_final_answer\"\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"hello\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "# Prompt the chat_agent\n",
    "response = run_chat_agent(message_history)\n",
    "\n",
    "# Run router_agent\n",
    "route = run_router_agent(response)\n",
    "\n",
    "print(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Zj3CJ1q_nGsz",
    "outputId": "dd48f886-b14b-43be-a08d-dd94537ed881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n"
     ]
    }
   ],
   "source": [
    "def run_research_agent(llm_response):\n",
    "\n",
    "    print(\"---RESEARCH AGENT---\")\n",
    "\n",
    "    # Extract the status\n",
    "    json_response = json.loads(llm_response)\n",
    "    action_dict = json_response['Action']\n",
    "    func_to_run = action_dict['function']\n",
    "    func_input_list = action_dict['input']\n",
    "    \n",
    "    answer_list = []\n",
    "\n",
    "    if func_to_run == \"find_arxiv_research_papers\":\n",
    "        for search_query in func_input_list:\n",
    "            answer = run_arxiv_search(search_query, top_k=5)\n",
    "            answer_list.append(answer)\n",
    "    else:\n",
    "        for search_query in func_input_list:\n",
    "            answer = run_tavily_search(search_query, num_results=5)\n",
    "            answer_list.append(answer)\n",
    "\n",
    "    print(\"func_to_run:\", func_to_run)\n",
    "    print(\"func_arg:\", func_input_list)\n",
    "    print(\"Output:\", answer_list)\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"Has OpenAi released any new open source models?\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "# Prompt the chat_agent\n",
    "response = run_chat_agent(message_history)\n",
    "\n",
    "# Run router_agent\n",
    "route = run_router_agent(response)\n",
    "\n",
    "\n",
    "if route == \"to_research_agent\":\n",
    "    answer = run_research_agent(response)\n",
    "\n",
    "    # Update message history\n",
    "    #message = {\"role\": \"user\", \"content\": f\"Observation: {answer}\"}\n",
    "    #message_history.append(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output_type(output):\n",
    "    try:\n",
    "        json.loads(output)\n",
    "        return \"is_json\"\n",
    "    except json.JSONDecodeError:\n",
    "        return \"is_plain_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Xrmd29mU7ZLa"
   },
   "outputs": [],
   "source": [
    "def run_final_answer_agent(llm_response):\n",
    "\n",
    "    # Check if the output is JSON\n",
    "    output_type = check_output_type(llm_response)\n",
    "\n",
    "    print(\"---FINAL ANSWER AGENT---\")\n",
    "\n",
    "    if output_type == 'is_json':\n",
    "\n",
    "        json_response = json.loads(llm_response)\n",
    "        final_answer = json_response['Answer']\n",
    "    \n",
    "        print(\"Final answer:\", final_answer)\n",
    "\n",
    "    # The model has ouput plain text that's\n",
    "    # not being formatted as per the system message.\n",
    "    else:\n",
    "        print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6enpJPmzLim"
   },
   "source": [
    "# Run the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "lSuoHVFDy8WZ",
    "outputId": "80272366-f203-4680-f6b0-7f4562f46626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the current year. Let me think. The user's question is straightforward. The current date is August 2025, so the answer should be 2025. I don't need to use any actions here because it's a simple question that requires only the current year. The answer is clear and doesn't need any external data. So I'll just provide the answer as specified.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: 2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = \"What is the current year?\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_input)\n",
    "\n",
    "for i in range(0,10):\n",
    "\n",
    "    # Prompt the chat_agent\n",
    "    llm_response = run_chat_agent(message_history)\n",
    "    \n",
    "    # Update message history\n",
    "    message = {\"role\": \"assistant\", \"content\": llm_response}\n",
    "    message_history.append(message)\n",
    "\n",
    "    # Run router_agent\n",
    "    route = run_router_agent(llm_response)\n",
    "\n",
    "\n",
    "    if route == \"to_research_agent\":\n",
    "\n",
    "        answer = run_research_agent(llm_response)\n",
    "        \n",
    "        user_input = f\"Observation: {answer}\"\n",
    "        message = {\"role\": \"user\", \"content\": user_input}\n",
    "        message_history.append(message)\n",
    "\n",
    "    else:\n",
    "\n",
    "        run_final_answer_agent(llm_response)\n",
    "\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN_-LCvuNrE_"
   },
   "source": [
    "# Run a chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "fZLpFq4tvmG7",
    "outputId": "e01afa16-4c1a-4029-a568-ae6838d2d096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user said \"Hello\". I need to respond politely. Let me check the guidelines. The first example shows a simple answer. So I should just reply with a friendly message. No need for any actions here. Just a standard greeting.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: Hello! How can I assist you today?\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  How are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond politely. Since they're asking about my well-being, I should say I'm fine and offer help. The previous example showed a JSON response with \"Answer\" and \"Status\". I'll follow that structure. No need for any actions here because it's a straightforward question. Just a simple reply.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: I'm fine, thanks. How can I assist you today?\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Has OpenAi released any new open source models recently?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user is asking if OpenAI has released any new open-source models recently. Let me think about how to approach this.\n",
      "\n",
      "First, I need to recall what I know about OpenAI's recent activities. OpenAI is known for releasing models like GPT-3, GPT-4, and others, but they've been cautious about open-source models, especially given their business model and security concerns. I remember that they have released some models under the GPT family, but they aren't typically open-source. For example, GPT-3.5 and GPT-4 are available through their API, but the code isn't open-source. \n",
      "\n",
      "Wait, the user specifically mentioned \"open-source models.\" OpenAI has a policy where they don't release the code for their models, even if they are publicly available. So, maybe there haven't been any new open-source models recently. But I should verify this. \n",
      "\n",
      "I should check the latest news or announcements from OpenAI. Since my knowledge cutoff is October 2023, I need to rely on that. As of that date, OpenAI hasn't released any new open-source models. They have focused on commercial models and APIs. However, there's the GPT-3.5 and GPT-4, but those are not open-source. \n",
      "\n",
      "Alternatively, maybe they have released some smaller models or tools under different names. But I don't recall any recent announcements. The user might be confused between models that are available via API and those that are open-source. \n",
      "\n",
      "So, the answer would be that OpenAI hasn't released any new open-source models recently. They have released new versions of their models, but they aren't open-source. I should also mention their policy on open-source models. \n",
      "\n",
      "I should also consider using the web search action to check for the latest information. But since the user is asking about something that's within my knowledge up to October 2023, and the current date is August 2025, there might be updates after that. However, since I can't access real-time data, I have to rely on my existing knowledge. \n",
      "\n",
      "Therefore, the answer is that as of the latest information, OpenAI hasn't released any new open-source models. They have released new models but they are not open-source. I should present this clearly.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: As of the latest information up to October 2023, OpenAI has not released any new open-source models. OpenAI focuses on commercial models like GPT-3.5 and GPT-4, which are available via APIs but not open-sourced. Their policy emphasizes security and commercial use over open-source distribution. For the most current updates, checking OpenAI's official announcements is recommended.\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Please run a web search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user is asking if OpenAI has released any new open-source models recently. My previous answer was based on information up to October ä¼šè¢«2023, but the current date is August 2025. I need to check for any updates since then.\n",
      "\n",
      "First, I should run a web search to get the latest information. The user wants a web search, so I'll use the run_web_search action. The keywords should be \"OpenAI new open source models recent\" to find the most up-to-date news or announcements. \n",
      "\n",
      "Wait, maybe I should also include \"2025\" in the search terms to ensure the results are from the latest period. Let me adjust the search query to \"OpenAI new open source models 2025\" to get the most relevant information. That should help find any announcements made in August 2025. \n",
      "\n",
      "I need to make sure the search is comprehensive. Maybe adding \"latest\" or \"announcements\" could help. But the user just said \"run a web search,\" so sticking to the main keywords. Let's proceed with \"OpenAI new open source models 2025\" as the search query.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: PAUSE\n",
      "Route: to_research_agent\n",
      "---RESEARCH AGENT---\n",
      "func_to_run: run_web_search\n",
      "func_arg: ['OpenAI new open source models 2025']\n",
      "Output: [{'query': 'OpenAI new open source models 2025', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.youtube.com/watch?v=1lYbTA5YYNw', 'title': 'OpenAI Finally Goes Open-Source: 120B & 20B Models', 'content': \"OpenAI Finally Goes Open-Source: 120B & 20B Models\\nPrompt Engineering\\n215000 subscribers\\n262 likes\\n9830 views\\n5 Aug 2025\\nLINKS:\\n\\nhttps://openai.com/open-models/\\nhttps://cookbook.openai.com/articles/gpt-oss/run-locally-ollama\\nhttps://openai.com/index/introducing-gpt-oss/\\nhttps://github.com/openai/harmony\\nhttps://github.com/openai/gpt-oss?tab=readme-ov-file#browser \\n\\nWebsite: https://engineerprompt.ai/\\n\\nRAG Beyond Basics Course:\\nhttps://prompt-s-site.thinkific.com/courses/rag\\n\\nLet's Connect: \\nðŸ¦¾ Discord: https://discord.com/invite/t4eYQRUcXB\\nâ˜• Buy me a Coffee: https://ko-fi.com/promptengineering\\n|ðŸ”´ Patreon: https://www.patreon.com/PromptEngineering\\nðŸ’¼Consulting: https://calendly.com/engineerprompt/consulting-call\\nðŸ“§ Business Contact: engineerprompt@gmail.com\\nBecome Member: http://tinyurl.com/y5h28s6h\\n\\nðŸ’» Pre-configured localGPT VM: https://bit.ly/localGPT (use Code: PromptEngineering for 50% off).  \\n\\nSignup for Newsletter, localgpt:\\nhttps://tally.so/r/3y9bb0\\n\\n00:00 Introduction to OpenAI's New Models\\n00:10 Model Specifications and Requirements\\n00:31 Capabilities and Features\\n01:10 Running the Models Locally\\n01:21 Performance Benchmarks\\n01:37 Safety and Ethical Considerations\\n03:11 Technical Details and Training\\n10:15 Community and Industry Collaboration\\n13:09 Hands-On Testing\\n15:05 Conclusion and Future Outlook\\n46 comments\\n\", 'score': 0.8752871, 'raw_content': None}, {'url': 'https://www.forbes.com/sites/paulbaier/2025/08/07/openais-new-open-source-models-are-a-very-big-deal-3-reasons-why/', 'title': 'OpenAI Releases Two Powerful Open Source AI Models', 'content': 'OpenAI Releases Two Powerful Open Source AI Models: What You Need To Know # OpenAI Releases Two Powerful Open Source AI Models: What You Need To Know OpenAIâ€™s release this week of two new open source models â€” gpt-oss-120b and gpt-oss-20b â€” is a major shift in the AI landscape. Until now, OpenAIâ€™s AI models have been closed source and expensive, especially compared to open source alternativesâ€”particularly those from China, which offer approximately 90% of the performance at 90% lower cost. On August 5, OpenAI released two new open source AI models: gpt-oss-120b (a 120-billion-parameter Mixture-of-Experts model) and gpt-oss-20b (a smaller, 20-billion-parameter model). * Encourage your AI team to test OpenAIâ€™s new open source models, especially if on-premise deployment is a priority.', 'score': 0.78846955, 'raw_content': None}, {'url': 'https://openai.com/open-models/', 'title': 'Open models by OpenAI', 'content': '*   ChatGPT(opens in a new window) *   Sora(opens in a new window) *   API Platform(opens in a new window) *   Open Models  *   Sora Log in(opens in a new window) Open models by OpenAI Advanced open-weight reasoning models to customize for any use case and run anywhere. Image 1 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-120b A large open model designed to run in data centers and on high-end desktops and laptops. Start building(opens in a new window) (opens in a new window)Image 2 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-20b A medium-sized open model that can run on most desktops and laptops. *   Sora log in(opens in a new window) *   API log in(opens in a new window)', 'score': 0.5122296, 'raw_content': None}, {'url': 'https://northflank.com/blog/self-host-openai-gpt-oss-120b-open-source-chatgpt', 'title': \"Run OpenAI's new GPT-OSS (open-source) model on ...\", 'content': \"Run OpenAI's new GPT-OSS (open-source) model on Northflank | Blog â€” Northflank Image 1: Header image for blog post: Run OpenAI's new GPT-OSS (open-source) model on Northflank Run OpenAI's new GPT-OSS (open-source) model on Northflank ðŸ‘‰ Deploy GPT-OSS-120B on Northflank now How to deploy GPT-OSS 120B on Northflank You can deploy GPT-OSS with Open WebUI using Northflankâ€™s stack template in one click. Deploy GPT-OSS via Northflank Northflank provides templates to deploy GPT-OSS with a few clicks. Deploy the stack to save the template in your team, then run it to create a cluster in Northflankâ€™s cloud with a vLLM service for high-performance inference and an Open WebUI for easy interaction. Use the public code.run URL from the service header (e.g., https://api--gpt-oss-vllm--abc123.code.run/v1/models) to check available models. MODEL=openai/gpt-oss-120b %20model%20on%20Northflank&url=https://northflank.com/blog/self-host-openai-gpt-oss-120b-open-source-chatgpt)%20model%20on%20Northflank&u=https://northflank.com/blog/self-host-openai-gpt-oss-120b-open-source-chatgpt)%20model%20on%20Northflank&url=https://northflank.com/blog/self-host-openai-gpt-oss-120b-open-source-chatgpt)\", 'score': 0.49348235, 'raw_content': None}, {'url': 'https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/', 'title': \"OpenAI launches two 'open' AI reasoning models\", 'content': \"OpenAI launches two 'open' AI reasoning models | TechCrunch OpenAI launches two 'open' AI reasoning models | TechCrunch OpenAI launches two â€˜openâ€™ AI reasoning models In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. AI, open model, OpenAI\", 'score': 0.4851368, 'raw_content': None}], 'response_time': 1.38, 'request_id': '57b7f389-de61-47e7-85da-ca49476ba4f0'}]\n",
      "<think>\n",
      "Okay, let's see. The user asked if OpenAI has released any new open-source models recently. I first checked my knowledge up to October 2023 and found no recent releases. Then the user wanted a web search, so I did a search for \"OpenAI new open source models 2025\". The results from the search show that in August 2025, OpenAI did release two new open-source models: gpt-oss-120b and gpt-oss-20b. The articles mention these models are large and medium-sized, designed for different use cases. They also provide links to official pages and deployment guides. Since the search results are from 2025, which is after my knowledge cutoff, I need to relay this information accurately. The user might be looking for the latest updates, so I should present the findings from the search, highlighting the models and their details. I should also mention the official resources for more info.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: As of August 2025, OpenAI has released two new open-source AI models: **gpt-oss-120b** (a 120-billion-parameter model for data centers and high-end hardware) and **gpt-oss-20b** (a 20-billion-parameter model for mainstream desktops/laptops). These models are part of OpenAI's shift toward open-source offerings, with details available on their official website [openai.com/open-models](https://openai.com/open-models). They can be deployed locally via tools like Northflank or Open WebUI. For technical specs and deployment guides, check the [GitHub repositories](https://github.com/openai/gpt-oss).\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  What are the latest Qwen models?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user is asking about the latest Qwen models. Let me start by recalling what I know about Qwen. Qwen is a series of large language models developed by Alibaba Cloud. The latest versions, as of my knowledge cutoff in October 2023, would be Qwen3 and Qwen3.5. But since the current date is August 2025, there might have been updates beyond that.\n",
      "\n",
      "Wait, the user's previous interaction was about OpenAI's new models in 2025. The assistant provided information about OpenAI releasing gpt-oss-120b and gpt-oss-20b. Now the user is switching to Qwen. I need to make sure I provide accurate information up to 2025.\n",
      "\n",
      "I should check if there have been any recent updates to Qwen after October 2023. However, since my knowledge is up to October 2023, I can't confirm anything beyond that. The user might be expecting information on the latest versions, but if there's no data beyond that, I should state that and suggest checking official sources.\n",
      "\n",
      "Also, the user might be confused between OpenAI and Qwen, so I need to clarify that Qwen is a different model series. I should mention the available versions as of October 2023 and advise them to visit Alibaba Cloud's website for the most current details. Maybe include some key features of the latest Qwen models if I have that information.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: As of October 2023, the latest Qwen models include **Qwen3** and **Qwen3.5**, which offer enhanced capabilities in natural language understanding, code generation, and multilingual support. For the most up-to-date information on newer versions or specific features, please check Alibaba Cloud's official documentation or announcements, as updates may have occurred since my knowledge cutoff in October 2023.\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Please run a web search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user asked about the latest Qwen models. I need to check if there's recent information beyond my knowledge cutoff in October 2023. Since the user is asking for the latest, I should run a web search to get up-to-date data.\n",
      "\n",
      "First, I'll use the run_web_search action with the query \"latest Qwen models 2025\". That should fetch the most recent information. Let me make sure the search terms are relevant. The results might include official announcements, blog posts, or tech news articles from 2025. \n",
      "\n",
      "After getting the search results, I'll need to parse the information to find details about new Qwen models. The user might be interested in model versions, features, release dates, or deployment options. I should check the sources for credibility, like official Alibaba Cloud pages or trusted tech websites. \n",
      "\n",
      "If the search results mention newer models like Qwen3.5 or Qwen4, or any updates in capabilities, I'll summarize that. Also, check if there are any changes in functionality, such as improved multilingual support, code generation, or integration with other tools. \n",
      "\n",
      "I need to present the information clearly, highlighting the key updates and providing links to official resources for further details. Make sure the answer is accurate and based on the latest available data from the search results.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: PAUSE\n",
      "Route: to_research_agent\n",
      "---RESEARCH AGENT---\n",
      "func_to_run: run_web_search\n",
      "func_arg: ['latest Qwen models 2025']\n",
      "Output: [{'query': 'latest Qwen models 2025', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Qwen', 'title': 'Qwen', 'content': 'On April 28, 2025, the Qwen 3 model family was released, with all models licensed under the Apache 2.0 license. The Qwen 3 model family includes both', 'score': 0.8576849, 'raw_content': None}, {'url': 'https://www.alibabacloud.com/help/en/model-studio/models', 'title': 'Alibaba Cloud Model Studio:Models and pricing', 'content': 'The latest qwen-turbo-2025-04-28 and qwen-turbo-latest models support both thinking and non-thinking modes. You can switch between the two modes using the', 'score': 0.85567063, 'raw_content': None}, {'url': 'https://dev.to/czmilo/2025-latest-complete-guide-to-qwen-image-edit-image-editing-model-2kd5', 'title': '2025 Latest: Complete Guide to Qwen-Image-Edit ...', 'content': \"Qwen-Image-Edit is the latest image editing foundation model released by Alibaba's Qwen team, built upon the 20B parameter Qwen-Image model.\", 'score': 0.84948516, 'raw_content': None}, {'url': 'https://www.alibabacloud.com/help/en/model-studio/what-is-qwen-llm', 'title': 'Qwen LLMs', 'content': 'Qwen3 models released in April, 2025, supports both thinking and non-thinking modes, allowing you to switch between the two using the', 'score': 0.8458536, 'raw_content': None}, {'url': 'https://qwen.ai/', 'title': 'Qwen', 'content': 'Qwen Chat is an AI assistant for everyone, powered by the Qwen series models. ... 2025/07/24. Here we introduce the latest update of Qwen-MT (qwen-mt-turbo)', 'score': 0.81524754, 'raw_content': None}], 'response_time': 2.75, 'request_id': '61ca9524-363f-437e-b692-5ea6990a2d98'}]\n",
      "<think>\n",
      "Okay, let's tackle this query about the latest Qwen models. The user asked, \"What are the latest Qwen models?\" and I need to provide an accurate answer based on the web search results.\n",
      "\n",
      "First, looking at the search results, there are several mentions of Qwen models from 2025. The Wikipedia entry says that on April 28, 2025, the Qwen 3 model family was released, licensed under Apache 2.0. That's a key point. Then, Alibaba Cloud's Model Studio mentions models like qwen-turbo-2025-04-28 and qwen-turbo-latest, which support thinking and non-thinking modes. The Dev.to article talks about Qwen-Image-Edit, an image editing model built on the 20B parameter Qwen-Image model. Another link mentions Qwen3 models from April 2025 supporting both modes. The Qwen AI website also notes updates to Qwen-MT in July 2025.\n",
      "\n",
      "So, compiling this info, the latest models include Qwen 3, which is a major release. There's also Qwen-Image-Edit for image editing. The turbo models from April 2025 and the MT updates in July are part of the latest releases. I should list these models, mention their features, and note the licensing and availability through Alibaba Cloud. Also, the user might want to know where to get more details, so including links to the official sources would be helpful. Need to present this in a clear, concise manner without markdown, using bullet points if needed, but since the answer should be in JSON, I'll structure it accordingly.\n",
      "</think>\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: As of 2025, the latest Qwen models include:\n",
      "\n",
      "1. **Qwen 3** (April 28, 2025): A major release with improved capabilities, licensed under Apache 2.0. It supports both thinking and non-thinking modes.\n",
      "2. **Qwen-Image-Edit**: A foundation model for image editing, built on the 20B-parameter Qwen-Image model.\n",
      "3. **Qwen-Turbo** (April 2025): Optimized for speed and efficiency, with support for thinking/non-thinking modes.\n",
      "4. **Qwen-MT** (July 2025): Updated version with enhanced multilingual and task-specific capabilities.\n",
      "\n",
      "For technical details and deployment, visit [Alibaba Cloud Model Studio](https://www.alibabacloud.com/help/en/model-studio/models) or the [Qwen official site](https://qwen.ai).\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Exiting the loop. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# NOTES\n",
    "# 1. The model thinking is being printed out.\n",
    "\n",
    "\n",
    "# Initialize the message history\n",
    "message_history = initialize_message_history(chat_agent_system_message)\n",
    "\n",
    "while True:\n",
    "\n",
    "    print()\n",
    "    print(\"==========\")\n",
    "    user_input = input(\"Enter something (or 'q' to quit): \")\n",
    "    print(\"==========\")\n",
    "\n",
    "    # Update message history\n",
    "    message = {\"role\": \"user\", \"content\": user_input}\n",
    "    message_history.append(message)\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"Exiting the loop. Goodbye!\")\n",
    "        break  # Exit the loop\n",
    "\n",
    "\n",
    "    for i in range(0,10):\n",
    "           \n",
    "        llm_response = run_chat_agent(message_history)\n",
    "        \n",
    "        # Update message history\n",
    "        message = {\"role\": \"assistant\", \"content\": llm_response}\n",
    "        message_history.append(message)\n",
    "\n",
    "        # Run router_agent\n",
    "        route = run_router_agent(llm_response)\n",
    "            \n",
    "\n",
    "        if route == \"to_research_agent\":\n",
    "\n",
    "            answer = run_research_agent(llm_response)\n",
    "            \n",
    "            user_input = f\"Observation: {answer}\"\n",
    "            message = {\"role\": \"user\", \"content\": user_input}\n",
    "            message_history.append(message)\n",
    "\n",
    "        else:\n",
    "\n",
    "            run_final_answer_agent(llm_response)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "pYqL1V1TJ_Yq"
   },
   "outputs": [],
   "source": [
    "# Model output formatting is a issue.\n",
    "# The model does not always format the output in the\n",
    "# specified JSON format.\n",
    "# I suspect that as the size of the chat history increases,\n",
    "# the models ability to follow system message instructions degrades.\n",
    "# This is an issue for both the 4B and the 14B models.\n",
    "\n",
    "#print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 8701001,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 164070052,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
