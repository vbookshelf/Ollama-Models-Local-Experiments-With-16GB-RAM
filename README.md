# Qwen3 with the Ollama Python Package - Local Experiments
My experiments using the Ollama python package to run Qwen3 4B, 8B and 14B locally on an M4 Macbook Air, 16GB RAM - in a jupyter notebook.
<br>
<br>

Qwen3 4B Ollama:
- parameters: 4.02B
- quantization: Q4_K_M
- size: 2.5GB

Qwen3 8B Ollama:
- parameters: 8.19B
- quantization: Q4_K_M
- size: 5.2GB

Qwen3 14B Ollama:
- parameters: 14.8B
- quantization: Q4_K_M
- size: 9.3GB

Qwen3 30B Ollama:
- parameters: 30.5B
- quantization: Q4_K_M
- size: 19GB
<br>

## Experiments

- Exp1 - Set up the UV project folder and Python environment on Mac<br>
(Everything is being run locally on the Mac desktop)<br>
https://github.com/vbookshelf/Qwen3-Ollama-Local-Experiments/tree/main/Exp1%20-%20Set%20up%20the%20UV%20project%20folder%20and%20python%20environment

- Exp2 - Simple Ollama Python code in a Jupyter notebook<br>
(Set up the code for chat inference)<br>
https://github.com/vbookshelf/Qwen3-Ollama-Local-Experiments/tree/main/Exp2%20-%20Simple%20ollama%20python%20code%20in%20a%20jupyter%20notebook

- Exp3 - Use Qwen3 models in a simple ReAct workflow<br>
(Using the simple "dog weights" example. Includes a python chat loop.)<br>
https://github.com/vbookshelf/Qwen3-Ollama-Local-Experiments/tree/main/Exp3%20-%20Use%20Qwen3%20in%20a%20simple%20ReAct%20lworkflow
