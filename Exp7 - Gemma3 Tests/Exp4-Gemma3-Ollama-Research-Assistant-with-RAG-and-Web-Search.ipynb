{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsE5rhFuCiQj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EdY2JYCQePWH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gemma3:12b'\n",
    "\n",
    "# The embeddings and the dataframe created and saved in Part 1\n",
    "PATH_TO_EMBEDS = 'compressed_array.npz'\n",
    "PATH_TO_DF = 'compressed_dataframe.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqxrj_T6C_z1"
   },
   "source": [
    "# Define the API clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Desktop/dot-env-api-keys/my-api-keys.env\"))\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PzAyFOnWC_d9"
   },
   "outputs": [],
   "source": [
    "# Tavily web search\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXpFe86y6DOF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwW76J3yJ5QN"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_message_history(system_message):\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1pl8ksbbeuSD"
   },
   "outputs": [],
   "source": [
    "def create_message_history(system_message, user_input):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a message history messages list.\n",
    "    Args:\n",
    "        system_message (str): The system message\n",
    "        user_query (str): The user input\n",
    "    Returns:\n",
    "        A list of dicts in OpenAi chat format\n",
    "    \"\"\"\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_input\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ywzSfgqFFOkZ"
   },
   "outputs": [],
   "source": [
    "def initialize_message_history(system_message):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a message history messages list.\n",
    "    Args:\n",
    "        system_message (str): The system message\n",
    "        user_query (str): The user input\n",
    "    Returns:\n",
    "        A list of dicts in OpenAi chat format\n",
    "    \"\"\"\n",
    "\n",
    "    message_history = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_message\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "    return message_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate the thinking and the response\n",
    "\n",
    "def process_response(text):\n",
    "    \n",
    "    text1 = text.split('</think>')[0]\n",
    "    text2 = text.split('</think>')[1]\n",
    "    \n",
    "    thinking_text = text1 + '</think>'\n",
    "    response_text = text2.strip()\n",
    "\n",
    "    return thinking_text, response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent_system_message = f\"\"\"\n",
    "You are a friendly and helpful research assistant named Serena.\n",
    "\n",
    "Your knowledge cutoff: August 2024\n",
    "Current date: August 2025\n",
    "\n",
    "1. You provide polite answers to simple questions.\n",
    "If the user's input requires only a simple answer, then output your answer as JSON.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: Hello. How are you?\n",
    "\n",
    "You output:\n",
    "\n",
    "{{\n",
    "\"Answer\": \"I'm fine, thanks.\",\n",
    "\"Status\": \"DONE\"\n",
    "}}\n",
    "\n",
    "2. You can also run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop, you output an Answer.\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "Output your response as a JSON string.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "find_arxiv_research_papers:\n",
    "e.g. find_arxiv_research_papers: [list of search keywords and phrases for a RAG search of the ArXiv database.]\n",
    "Returns research papers from the ArXiv database.\n",
    "\n",
    "run_web_search:\n",
    "e.g. run_web_search: [list of search keywords and phrases for a web search]\n",
    "Returns text content from search results.\n",
    "\n",
    "You can only call one action at a time.\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What are the latest techniques for detecting Pneumonia on x-rays using AI?\n",
    "{{\n",
    "\"Thought\": \"I should look for relevant research papers in the ArXiv database by using find_arxiv_research_papers.\",\n",
    "\"Action\": {{\"function\":\"find_arxiv_research_papers\", \"input\": [\"Pneumonia detection with AI\", \"Computer vision\", \"Object detection\"]}},\n",
    "\"Status\": \"PAUSE\"\n",
    "}}\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: <results>A list of research papers and their content</results>\n",
    "\n",
    "You then output:\n",
    "{{\n",
    "\"Answer\": \"Your final report.\",\n",
    "\"Status\": \"DONE\"\n",
    "}}\n",
    "\n",
    "ALWAYS check your responses to ensure that all required JSON keys and values have been included.\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0reljPhbezOM"
   },
   "source": [
    "# Set up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Molly! 😊\n"
     ]
    }
   ],
   "source": [
    "def make_llm_api_call(message_history):\n",
    "\n",
    "    model_name = MODEL_NAME\n",
    "\n",
    "    response: ChatResponse = chat(model=model_name, \n",
    "                                  messages=message_history,\n",
    "                                )\n",
    "\n",
    "    output_text = response['message']['content']\n",
    "\n",
    "    #thinking_text, response_text = process_response(output_text)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "system_message = \"Your name is Molly.\"\n",
    "user_message = \"What's your name?\"\n",
    "\n",
    "message_history = create_message_history(system_message, user_message)\n",
    "\n",
    "response_text = make_llm_api_call(message_history)\n",
    "\n",
    "print(response_text)\n",
    "\n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShdYyu_Ne7R2"
   },
   "source": [
    "## Set up the tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiv RAG search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_faiss_search(query_text, top_k):\n",
    "    \n",
    "    # Run FAISS exhaustive search\n",
    "    \n",
    "    query = [query_text]\n",
    "\n",
    "    # Vectorize the query string\n",
    "    query_embedding = sent_model.encode(query)\n",
    "\n",
    "    # Run the query\n",
    "    # index_vals refers to the chunk_list index values\n",
    "    scores, index_vals = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get the list of index vals\n",
    "    index_vals_list = index_vals[0]\n",
    "    \n",
    "    return index_vals_list\n",
    "    \n",
    "\n",
    "def run_rerank(index_vals_list, query_text):\n",
    "    \n",
    "    chunk_list = list(df_data['prepared_text'])\n",
    "\n",
    "    # Replace the chunk index values with the corresponding strings\n",
    "    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n",
    "\n",
    "    # Format the input for the cross encoder\n",
    "    # The input to the cross_encoder is a list of lists\n",
    "    # [[query_text, pred_text1], [query_text, pred_text2], ...]\n",
    "\n",
    "    cross_input_list = []\n",
    "\n",
    "    for item in pred_strings_list:\n",
    "\n",
    "        new_list = [query_text, item]\n",
    "\n",
    "        cross_input_list.append(new_list)\n",
    "\n",
    "\n",
    "    # Put the pred text into a dataframe\n",
    "    df = pd.DataFrame(cross_input_list, columns=['query_text', 'pred_text'])\n",
    "\n",
    "    # Save the orginal index (i.e. df_data index values)\n",
    "    df['original_index'] = index_vals_list\n",
    "\n",
    "    # Now, score all retrieved passages using the cross_encoder\n",
    "    cross_scores = cross_encoder.predict(cross_input_list)\n",
    "\n",
    "    # Add the scores to the dataframe\n",
    "    df['cross_scores'] = cross_scores\n",
    "\n",
    "    # Sort the DataFrame in descending order based on the scores\n",
    "    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n",
    "    \n",
    "    # Reset the index (*This was missed previously*)\n",
    "    df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for i in range(0,len(df_sorted)):\n",
    "\n",
    "        text = df_sorted.loc[i, 'pred_text']\n",
    "\n",
    "        # Get the arxiv id\n",
    "        # original_index refers to the index values in df_filtered\n",
    "        original_index = df_sorted.loc[i, 'original_index']\n",
    "        arxiv_id = df_data.loc[original_index, 'id']\n",
    "        cat_text = df_data.loc[original_index, 'cat_text']\n",
    "        title = df_data.loc[original_index, 'title']\n",
    "\n",
    "        # Crete the link to the research paper pdf\n",
    "        link_to_pdf = f'https://arxiv.org/pdf/{arxiv_id}'\n",
    "\n",
    "        item = {\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'link_to_pdf': link_to_pdf,\n",
    "            'cat_text': cat_text,\n",
    "            'title': title,\n",
    "            'abstract': text\n",
    "        }\n",
    "\n",
    "        pred_list.append(item)\n",
    "\n",
    "    return pred_list\n",
    "\n",
    "\n",
    "def print_search_results(pred_list, num_results_to_print):\n",
    "    \n",
    "    for i in range(0,num_results_to_print):\n",
    "        \n",
    "        pred_dict = pred_list[i]\n",
    "        \n",
    "        link_to_pdf = pred_dict['link_to_pdf']\n",
    "        abstract = pred_dict['abstract']\n",
    "        cat_text = pred_dict['cat_text']\n",
    "        title = pred_dict['title']\n",
    "\n",
    "        print('Title:',title)\n",
    "        print('Categories:',cat_text)\n",
    "        print('Abstract:',abstract)\n",
    "        print('Link to pdf:',link_to_pdf)\n",
    "        print()\n",
    "    \n",
    "   \n",
    "def run_arxiv_search(query_text, top_k=50):\n",
    "    \n",
    "    # Run a faiss greedy search\n",
    "    pred_index_list = run_faiss_search(query_text, top_k)\n",
    "\n",
    "    # This returns a list of dicts with length equal to top_k\n",
    "    pred_list = run_rerank(pred_index_list, query_text)\n",
    "    \n",
    "    # Print the results\n",
    "    #print_search_results(pred_list, num_results_to_print)\n",
    "    \n",
    "    return pred_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2421966, 384)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed array\n",
    "embeddings = np.load(PATH_TO_EMBEDS)\n",
    "\n",
    "# Access the array by the name you specified ('my_array' in this case)\n",
    "embeddings = embeddings['array_data']\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/mv2kfll17l1gh1p7h1nny_940000gn/T/ipykernel_36655/2541750260.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2421966, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>cat_text</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturbati...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>High Energy Physics - Phenomenology</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pe...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>Combinatorics, Computational Geometry</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions {titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is describe...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>General Physics</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle n...</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>Combinatorics</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\Lam...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>Classical Analysis and ODEs, Functional Analysis</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \\\n",
       "0  A fully differential calculation in perturbati...           hep-ph   \n",
       "1  We describe a new algorithm, the $(k,\\ell)$-pe...    math.CO cs.CG   \n",
       "2  The evolution of Earth-Moon system is describe...   physics.gen-ph   \n",
       "3  We show that a determinant of Stirling cycle n...          math.CO   \n",
       "4  In this paper we show how to compute the $\\Lam...  math.CA math.FA   \n",
       "\n",
       "                                           cat_text  \\\n",
       "0               High Energy Physics - Phenomenology   \n",
       "1             Combinatorics, Computational Geometry   \n",
       "2                                   General Physics   \n",
       "3                                     Combinatorics   \n",
       "4  Classical Analysis and ODEs, Functional Analysis   \n",
       "\n",
       "                                       prepared_text  \n",
       "0  Calculation of prompt diphoton production cros...  \n",
       "1  Sparsity-certifying Graph Decompositions {titl...  \n",
       "2  The evolution of the Earth-Moon system based o...  \n",
       "3  A determinant of Stirling cycle numbers counts...  \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the compressed DataFrame\n",
    "\n",
    "df_data = pd.read_csv(PATH_TO_DF, compression='gzip')\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "embed_length = embeddings.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(embed_length)\n",
    "\n",
    "# Add the embeddings to the index\n",
    "faiss_index.add(embeddings)\n",
    "\n",
    "faiss_index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialize sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sent_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Initialize the cross_encoder for reranking\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "query_text = \"\"\"\n",
    "I want to build an invisibility cloak like the one in Harry Potter.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# RUN THE SEARCH\n",
    "num_results_to_print = 20 # top_k = 300\n",
    "pred_list = run_arxiv_search(query_text, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': 1101.0904,\n",
       " 'link_to_pdf': 'https://arxiv.org/pdf/1101.0904',\n",
       " 'cat_text': 'Classical Physics',\n",
       " 'title': \"Harry Potter's Cloak\",\n",
       " 'abstract': 'Harry Potter\\'s Cloak {title} The magic \"Harry Potter\\'s cloak\" has been the dream of human beings for really long time. Recently, transformation optics inspired from the advent of metamaterials offers great versatility for manipulating wave propagation at will to create amazing illusion effects. In the present work, we proposed a novel transformation recipe, in which the cloaking shell somehow behaves like a \"cloaking lens\", to provide almost all desired features one can expect for a real magic cloak. The most exciting feature of the current recipe is that an object with arbitrary characteristics (e.g., size, shape or material properties) can be invisibilized perfectly with positive-index materials, which significantly benefits the practical realization of a broad-band cloaking device fabricated with existing materials. Moreover, the one concealed in the hidden region is able to undistortedly communicate with the surrounding world, while the lens-like cloaking shell will protect the cloaked source/sensor from being traced back by outside detectors by creating a virtual image.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Who is the current UK Prime Minister?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.gov.uk/government/ministers/prime-minister', 'title': 'Prime Minister - GOV.UK', 'content': 'Current role holder. The Rt Hon Sir Keir Starmer KCB KC MP. Sir Keir Starmer became Prime Minister on 5 July 2024. Education. Keir attended Reigate Grammar', 'score': 0.8810534, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Prime_Minister_of_the_United_Kingdom', 'title': 'Prime Minister of the United Kingdom - Wikipedia', 'content': \"Prime Minister of the United Kingdom ; Incumbent Keir Starmer. since 5 July 2024 ; Government of the United Kingdom · Prime Minister's Office\", 'score': 0.87782305, 'raw_content': None}], 'response_time': 1.33, 'request_id': 'f119b82e-a400-4762-8cdf-2190798e975d'}\n"
     ]
    }
   ],
   "source": [
    "def run_tavily_search(query, num_results=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses the Tavily API to run a web search\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "        num_results (int): Num search results\n",
    "    Returns:\n",
    "        tav_response (json string): The search results in json format\n",
    "    \"\"\"\n",
    "\n",
    "    # For basic search:\n",
    "    tav_response = tavily_client.search(query=query, max_results=num_results)\n",
    "\n",
    "    return tav_response\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "query = \"Who is the current UK Prime Minister?\"\n",
    "\n",
    "results = run_tavily_search(query, num_results=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGZoKXk4fDX4"
   },
   "source": [
    "# Set up the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "{\n",
      "\"Answer\": \"Hello! How can I help you today?\",\n",
      "\"Status\": \"DONE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_chat_agent(message_history):\n",
    "\n",
    "    print(\"---CHAT AGENT---\")\n",
    "\n",
    "    # Prompt the llm\n",
    "    response = make_llm_api_call(message_history)\n",
    "\n",
    "    response = response.replace('```json', '')\n",
    "    response = response.replace('```', '')\n",
    "    response = response.strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"Hello\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "response_text = run_chat_agent(message_history)\n",
    "\n",
    "print(response_text)\n",
    "\n",
    "#print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NHOvD12NfDHX",
    "outputId": "25d189dc-9de0-4fcc-c198-833ccbd1e470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "to_final_answer\n"
     ]
    }
   ],
   "source": [
    "def run_router_agent(llm_response):\n",
    "\n",
    "    \"\"\"\n",
    "    Route to web search or not.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTER AGENT---\")\n",
    "\n",
    "    # Extract the status\n",
    "    json_response = json.loads(llm_response)\n",
    "    status = json_response['Status']\n",
    "    \n",
    "    print(\"Status:\", status)\n",
    "\n",
    "    if status == 'PAUSE':\n",
    "        print(\"Route: to_research_agent\")\n",
    "        return \"to_research_agent\"\n",
    "\n",
    "    else:\n",
    "        print(\"Route: to_final_answer\")\n",
    "        return \"to_final_answer\"\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"hello\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "# Prompt the chat_agent\n",
    "response = run_chat_agent(message_history)\n",
    "\n",
    "# Run router_agent\n",
    "route = run_router_agent(response)\n",
    "\n",
    "print(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Zj3CJ1q_nGsz",
    "outputId": "dd48f886-b14b-43be-a08d-dd94537ed881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: PAUSE\n",
      "Route: to_research_agent\n",
      "---RESEARCH AGENT---\n",
      "func_to_run: run_web_search\n",
      "func_arg: ['OpenAI open source models', 'OpenAI released open source models', 'OpenAI open source AI models']\n",
      "Output: [{'query': 'OpenAI open source models', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://openai.com/open-models/', 'title': 'Open models by OpenAI', 'content': '*   ChatGPT(opens in a new window) *   Sora(opens in a new window) *   API Platform(opens in a new window) *   Open Models  *   Sora Log in(opens in a new window) Open models by OpenAI Advanced open-weight reasoning models to customize for any use case and run anywhere. Image 1 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-120b A large open model designed to run in data centers and on high-end desktops and laptops. Start building(opens in a new window) (opens in a new window)Image 2 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-20b A medium-sized open model that can run on most desktops and laptops. *   Sora log in(opens in a new window) *   API log in(opens in a new window)', 'score': 0.98502, 'raw_content': None}, {'url': 'https://openai.com/index/introducing-gpt-oss/', 'title': 'Introducing gpt-oss', 'content': 'In addition to running the models through comprehensive safety training and evaluations, we also introduced an additional layer of evaluation by testing an adversarially fine-tuned version of gpt-oss-120b under our Preparedness Framework\\u2060(opens in a new window). Our objective was to align the models with the OpenAI Model Spec\\u2060(opens in a new window) and teach it to apply CoT reasoning\\u2060 and tool use before producing its answer. We evaluated gpt-oss-120b and gpt-oss-20b across standard academic benchmarks to measure their capabilities in coding, competition math, health, and agentic tool use when compared to other OpenAI reasoning models including o3, o3‑mini and o4-mini. You\\'re OpenAI\\'s newest open-weight language model gpt-oss-120b! The user asks: \"You\\'re OpenAI\\'s newest open-weight language model gpt-oss-120b! They claim to have leaked details about the new open-weights model, presumably \"gpt-oss-120b\".', 'score': 0.98169, 'raw_content': None}, {'url': 'https://platform.openai.com/docs/models', 'title': 'Models - OpenAI API', 'content': 'Models ; gpt-oss-120b. Most powerful open-weight model, fits into an H100 GPU ; gpt-oss-20b. Medium-sized open-weight model for low latency.', 'score': 0.98073, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=IJx_G3IjamE', 'title': 'Install OpenAI Open Source models on your PC and Visual ...', 'content': \"Download and configure OpenAI's medium-sized AI model on your machine. Interact with the model through a graphical interface or the command\", 'score': 0.97487, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=c7rjTEH-crQ', 'title': 'New FREE Open Source Models from OpenAI', 'content': '... OpenAI open source models (GPT-OSS): https://gpt-oss.com/. New FREE Open Source Models from OpenAI: Absolutely insane, on par with o3!', 'score': 0.96954, 'raw_content': None}], 'response_time': 1.05, 'request_id': 'd1e6a0e5-06c6-443e-adfe-34f5ac681f9e'}, {'query': 'OpenAI released open source models', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/', 'title': \"OpenAI launches two 'open' AI reasoning models | TechCrunch\", 'content': \"OpenAI launches two 'open' AI reasoning models | TechCrunch OpenAI launches two 'open' AI reasoning models | TechCrunch In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. The company also says its open model was trained using high-compute reinforcement learning (RL) — a post-training process to teach AI models right from wrong in simulated environments using large clusters of Nvidia GPUs. This was also used to train OpenAI’s o-series of models, and the open models have a similar chain-of-thought process in which they take additional time and computational resources to work through their answers. As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. AI, open model, OpenAI\", 'score': 0.98549, 'raw_content': None}, {'url': 'https://openai.com/open-models/', 'title': 'Open models by OpenAI', 'content': '*   ChatGPT(opens in a new window) *   Sora(opens in a new window) *   API Platform(opens in a new window) *   Open Models  *   Sora Log in(opens in a new window) Open models by OpenAI Advanced open-weight reasoning models to customize for any use case and run anywhere. Image 1 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-120b A large open model designed to run in data centers and on high-end desktops and laptops. Start building(opens in a new window) (opens in a new window)Image 2 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-20b A medium-sized open model that can run on most desktops and laptops. *   Sora log in(opens in a new window) *   API log in(opens in a new window)', 'score': 0.98355, 'raw_content': None}, {'url': 'https://openai.com/index/introducing-gpt-oss/', 'title': 'Introducing gpt-oss - OpenAI', 'content': 'In addition to running the models through comprehensive safety training and evaluations, we also introduced an additional layer of evaluation by testing an adversarially fine-tuned version of gpt-oss-120b under our Preparedness Framework\\u2060(opens in a new window). Our objective was to align the models with the OpenAI Model Spec\\u2060(opens in a new window) and teach it to apply CoT reasoning\\u2060 and tool use before producing its answer. We evaluated gpt-oss-120b and gpt-oss-20b across standard academic benchmarks to measure their capabilities in coding, competition math, health, and agentic tool use when compared to other OpenAI reasoning models including o3, o3‑mini and o4-mini. You\\'re OpenAI\\'s newest open-weight language model gpt-oss-120b! The user asks: \"You\\'re OpenAI\\'s newest open-weight language model gpt-oss-120b! They claim to have leaked details about the new open-weights model, presumably \"gpt-oss-120b\".', 'score': 0.97735, 'raw_content': None}, {'url': 'https://platform.openai.com/docs/models', 'title': 'Models - OpenAI API', 'content': 'Models. Explore all available models and compare their capabilities. Compare models. Featured models. GPT-5. The best model for coding and agentic tasks', 'score': 0.97539, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=BZ0qajzteqA', 'title': \"OpenAI's open source models are finally here - YouTube\", 'content': 'OpenAI’s open source models are finally here\\nTheo - t3․gg\\n468000 subscribers\\n3034 likes\\n89304 views\\n6 Aug 2025\\nThe open source OpenAI models are finally here, and oh boy were these worth the wait...\\n\\nThank you Agentuity for sponsoring! Check them out at: https://soydev.link/agentuity\\n\\nUse code THANKS-OPENAI for 1 month of T3 chat for $1: https://soydev.link/chat\\n\\nWant to sponsor a video? Learn more here: https://soydev.link/sponsor-me\\n\\nCheck out my Twitch, Twitter, Discord more at https://t3.gg/\\n\\nS/O Ph4se0n3 for the awesome edit 🙏\\n404 comments\\n', 'score': 0.96826, 'raw_content': None}], 'response_time': 1.12, 'request_id': '6bd74ac9-106a-4e39-9988-86accebe2e34'}, {'query': 'OpenAI open source AI models', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://openai.com/open-models/', 'title': 'Open models by OpenAI', 'content': '*   ChatGPT(opens in a new window) *   Sora(opens in a new window) *   API Platform(opens in a new window) *   Open Models  *   Sora Log in(opens in a new window) Open models by OpenAI Advanced open-weight reasoning models to customize for any use case and run anywhere. Image 1 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-120b A large open model designed to run in data centers and on high-end desktops and laptops. Start building(opens in a new window) (opens in a new window)Image 2 -0.55 0.16 0.09 0.10 -0.58 ### gpt-oss-20b A medium-sized open model that can run on most desktops and laptops. *   Sora log in(opens in a new window) *   API log in(opens in a new window)', 'score': 0.98595, 'raw_content': None}, {'url': 'https://platform.openai.com/docs/models', 'title': 'Models - OpenAI API', 'content': 'Models. Explore all available models and compare their capabilities. Compare models. Featured models. GPT-5. The best model for coding and agentic tasks', 'score': 0.98356, 'raw_content': None}, {'url': 'https://openai.com/index/introducing-gpt-oss/', 'title': 'Introducing gpt-oss - OpenAI', 'content': 'In addition to running the models through comprehensive safety training and evaluations, we also introduced an additional layer of evaluation by testing an adversarially fine-tuned version of gpt-oss-120b under our Preparedness Framework\\u2060(opens in a new window). Our objective was to align the models with the OpenAI Model Spec\\u2060(opens in a new window) and teach it to apply CoT reasoning\\u2060 and tool use before producing its answer. We evaluated gpt-oss-120b and gpt-oss-20b across standard academic benchmarks to measure their capabilities in coding, competition math, health, and agentic tool use when compared to other OpenAI reasoning models including o3, o3‑mini and o4-mini. You\\'re OpenAI\\'s newest open-weight language model gpt-oss-120b! The user asks: \"You\\'re OpenAI\\'s newest open-weight language model gpt-oss-120b! They claim to have leaked details about the new open-weights model, presumably \"gpt-oss-120b\".', 'score': 0.97868, 'raw_content': None}, {'url': 'https://openai.com/global-affairs/open-weights-and-ai-for-all/', 'title': 'Open weights and AI for all | OpenAI', 'content': 'Open weights and AI for all | OpenAI *   ChatGPT(opens in a new window) For swing states, the availability of powerful open-weight models will be an incentive to build on democratic AI and not autocratic AI. The White House’s new AI Action Plan rightly emphasizes that open-weight models can strengthen America’s AI leadership, support academic research, and expand across industry and government. As we noted in our March submission\\u2060(opens in a new window) to the White House Office of Science and Technology Policy to help inform the new AI Action Plan: “We believe the question of whether AI should be open or closed source is a false choice—we need both, and they can work in a complementary way that encourages the building of AI on American rails.”', 'score': 0.97274, 'raw_content': None}, {'url': 'https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/', 'title': \"OpenAI launches two 'open' AI reasoning models | TechCrunch\", 'content': \"OpenAI launches two 'open' AI reasoning models | TechCrunch OpenAI launches two 'open' AI reasoning models | TechCrunch In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. The company also says its open model was trained using high-compute reinforcement learning (RL) — a post-training process to teach AI models right from wrong in simulated environments using large clusters of Nvidia GPUs. This was also used to train OpenAI’s o-series of models, and the open models have a similar chain-of-thought process in which they take additional time and computational resources to work through their answers. As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. AI, open model, OpenAI\", 'score': 0.97068, 'raw_content': None}], 'response_time': 1.13, 'request_id': '1532276f-d9c6-4b6c-b0ed-3ec6804ce759'}]\n"
     ]
    }
   ],
   "source": [
    "def run_research_agent(llm_response):\n",
    "\n",
    "    print(\"---RESEARCH AGENT---\")\n",
    "\n",
    "    # Extract the status\n",
    "    json_response = json.loads(llm_response)\n",
    "    action_dict = json_response['Action']\n",
    "    func_to_run = action_dict['function']\n",
    "    func_input_list = action_dict['input']\n",
    "    \n",
    "    answer_list = []\n",
    "\n",
    "    if func_to_run == \"find_arxiv_research_papers\":\n",
    "        for search_query in func_input_list:\n",
    "            answer = run_arxiv_search(search_query, top_k=5)\n",
    "            answer_list.append(answer)\n",
    "    else:\n",
    "        for search_query in func_input_list:\n",
    "            answer = run_tavily_search(search_query, num_results=5)\n",
    "            answer_list.append(answer)\n",
    "\n",
    "    print(\"func_to_run:\", func_to_run)\n",
    "    print(\"func_arg:\", func_input_list)\n",
    "    print(\"Output:\", answer_list)\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "\n",
    "user_message = \"Has OpenAi released any new open source models?\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_message)\n",
    "\n",
    "# Prompt the chat_agent\n",
    "response = run_chat_agent(message_history)\n",
    "\n",
    "# Run router_agent\n",
    "route = run_router_agent(response)\n",
    "\n",
    "\n",
    "if route == \"to_research_agent\":\n",
    "    answer = run_research_agent(response)\n",
    "\n",
    "    # Update message history\n",
    "    #message = {\"role\": \"user\", \"content\": f\"Observation: {answer}\"}\n",
    "    #message_history.append(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output_type(output):\n",
    "    try:\n",
    "        json.loads(output)\n",
    "        return \"is_json\"\n",
    "    except json.JSONDecodeError:\n",
    "        return \"is_plain_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Xrmd29mU7ZLa"
   },
   "outputs": [],
   "source": [
    "def run_final_answer_agent(llm_response):\n",
    "\n",
    "    # Check if the output is JSON\n",
    "    output_type = check_output_type(llm_response)\n",
    "\n",
    "    print(\"---FINAL ANSWER AGENT---\")\n",
    "\n",
    "    if output_type == 'is_json':\n",
    "\n",
    "        json_response = json.loads(llm_response)\n",
    "        final_answer = json_response['Answer']\n",
    "    \n",
    "        print(\"Final answer:\", final_answer)\n",
    "\n",
    "    # The model has ouput plain text that's\n",
    "    # not being formatted as per the system message.\n",
    "    else:\n",
    "        print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6enpJPmzLim"
   },
   "source": [
    "# Run the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lSuoHVFDy8WZ",
    "outputId": "80272366-f203-4680-f6b0-7f4562f46626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: The current year is 2025.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = \"What is the current year?\"\n",
    "\n",
    "message_history = create_message_history(chat_agent_system_message, user_input)\n",
    "\n",
    "for i in range(0,10):\n",
    "\n",
    "    # Prompt the chat_agent\n",
    "    llm_response = run_chat_agent(message_history)\n",
    "    \n",
    "    # Update message history\n",
    "    message = {\"role\": \"assistant\", \"content\": llm_response}\n",
    "    message_history.append(message)\n",
    "\n",
    "    # Run router_agent\n",
    "    route = run_router_agent(llm_response)\n",
    "\n",
    "\n",
    "    if route == \"to_research_agent\":\n",
    "\n",
    "        answer = run_research_agent(llm_response)\n",
    "        \n",
    "        user_input = f\"Observation: {answer}\"\n",
    "        message = {\"role\": \"user\", \"content\": user_input}\n",
    "        message_history.append(message)\n",
    "\n",
    "    else:\n",
    "\n",
    "        run_final_answer_agent(llm_response)\n",
    "\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN_-LCvuNrE_"
   },
   "source": [
    "# Run a chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fZLpFq4tvmG7",
    "outputId": "e01afa16-4c1a-4029-a568-ae6838d2d096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  Hi Serena\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "---CHAT AGENT---\n",
      "---ROUTER AGENT---\n",
      "Status: DONE\n",
      "Route: to_final_answer\n",
      "---FINAL ANSWER AGENT---\n",
      "Final answer: Hello! I'm doing well, thank you for asking. How can I help you today?\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something (or 'q' to quit):  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Exiting the loop. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# NOTES\n",
    "# 1. The model thinking is being printed out.\n",
    "\n",
    "\n",
    "# Initialize the message history\n",
    "message_history = initialize_message_history(chat_agent_system_message)\n",
    "\n",
    "while True:\n",
    "\n",
    "    print()\n",
    "    print(\"==========\")\n",
    "    user_input = input(\"Enter something (or 'q' to quit): \")\n",
    "    print(\"==========\")\n",
    "\n",
    "    # Update message history\n",
    "    message = {\"role\": \"user\", \"content\": user_input}\n",
    "    message_history.append(message)\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"Exiting the loop. Goodbye!\")\n",
    "        break  # Exit the loop\n",
    "\n",
    "\n",
    "    for i in range(0,10):\n",
    "           \n",
    "        llm_response = run_chat_agent(message_history)\n",
    "        \n",
    "        # Update message history\n",
    "        message = {\"role\": \"assistant\", \"content\": llm_response}\n",
    "        message_history.append(message)\n",
    "\n",
    "        # Run router_agent\n",
    "        route = run_router_agent(llm_response)\n",
    "            \n",
    "\n",
    "        if route == \"to_research_agent\":\n",
    "\n",
    "            answer = run_research_agent(llm_response)\n",
    "            \n",
    "            user_input = f\"Observation: {answer}\"\n",
    "            message = {\"role\": \"user\", \"content\": user_input}\n",
    "            message_history.append(message)\n",
    "\n",
    "        else:\n",
    "\n",
    "            run_final_answer_agent(llm_response)\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a friendly and helpful research assistant named Serena.\\n\\nYour knowledge cutoff: August 2024\\nCurrent date: August 2025\\n\\n1. You provide polite answers to simple questions.\\nIf the user\\'s input requires only a simple answer, then output your answer as JSON.\\n\\nExample session:\\n\\nQuestion: Hello. How are you?\\n\\nYou output:\\n\\n{\\n\"Answer\": \"I\\'m fine, thanks.\",\\n\"Status\": \"DONE\"\\n}\\n\\n2. You can also run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop, you output an Answer.\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\nOutput your response as a JSON string.\\n\\nYour available actions are:\\n\\nfind_arxiv_research_papers:\\ne.g. find_arxiv_research_papers: [list of search keywords and phrases for a RAG search of the ArXiv database.]\\nReturns research papers from the ArXiv database.\\n\\nrun_web_search:\\ne.g. run_web_search: [list of search keywords and phrases for a web search]\\nReturns text content from search results.\\n\\nYou can only call one action at a time.\\n\\nExample session:\\n\\nQuestion: What are the latest techniques for detecting Pneumonia on x-rays using AI?\\n{\\n\"Thought\": \"I should look for relevant research papers in the ArXiv database by using find_arxiv_research_papers.\",\\n\"Action\": {\"function\":\"find_arxiv_research_papers\", \"input\": [\"Pneumonia detection with AI\", \"Computer vision\", \"Object detection\"]},\\n\"Status\": \"PAUSE\"\\n}\\n\\nYou will be called again with this:\\n\\nObservation: <results>A list of research papers and their content</results>\\n\\nYou then output:\\n{\\n\"Answer\": \"Your final report.\",\\n\"Status\": \"DONE\"\\n}\\n\\nALWAYS check your responses to ensure that all required JSON keys and values have been included.'},\n",
       " {'role': 'user', 'content': 'Hi Serena'},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\\n\"Answer\": \"Hello! I\\'m doing well, thank you for asking. How can I help you today?\",\\n\"Status\": \"DONE\"\\n}'},\n",
       " {'role': 'user', 'content': 'q'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "pYqL1V1TJ_Yq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"Qwen models 2025\",\n",
      "  \"response\": {\n",
      "    \"summary\": \"The Qwen models have seen significant development in 2025. Key highlights include the release of Qwen2.5-Max, Qwen2.5-VL (capable of analyzing charts, extracting data from scans, and understanding long videos), and advancements in Qwen3 (including versions with long-context understanding and enhanced capabilities).  Qwen2.5-VL is available for testing and download, while Qwen2.5-Max can be accessed through Qwen Chat.  Qwen models are actively being improved, with a focus on capabilities like controlling PCs and phones, and improved performance across various benchmarks. The models also have some restrictions on topics due to their origin.\",\n",
      "    \"key_releases\": [\n",
      "      \"Qwen2.5-Max: Large-scale MoE model, available in Qwen Chat.\",\n",
      "      \"Qwen2.5-VL:  Analyzes charts, extracts data, understands long videos.\",\n",
      "      \"Qwen3:  Long-context understanding, enhanced capabilities (e.g., Qwen3-30B-A3B-Thinking-2507).\"\n",
      "    ],\n",
      "    \"availability\": \"Qwen2.5-VL is available for testing and download; Qwen2.5-Max accessible via Qwen Chat.\",\n",
      "    \"limitations\": \"Some topic restrictions exist due to the models' origin.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 8701001,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 164070052,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
