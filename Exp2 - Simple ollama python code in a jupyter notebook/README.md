## Exp2 - Simple Ollama Python code in a Jupyter notebook

### Objective
- Run the Ollama chat code in a Jupyter notebook
- Set up code for non-streaming, streaming and running a python input chat loop


### Notes
- Remember that these models via Ollama are quantized
- All models (4B, 8B, and 14B) run fast on Mac CPU
- Even tho the 30B model size is larger than the 16GB RAM on the Mac, the model can still run, but very slowly.
- The models perform better with the thinking mode on.
- The overall "feel" of the Ollama Qwen models is great - runs acceptably fast on CPU and inspires confidence.
